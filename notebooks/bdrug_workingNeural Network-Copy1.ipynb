{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what do i need to do?\n",
    "\n",
    "I need to:\n",
    "    * make a basic gan out of biodeg\n",
    "    * make a basic gan out of druglike\n",
    "    * figure out the predictors that are being used for biodegradability\n",
    "        * try out those descriptors for predictive power on the druglike set \n",
    "        * (also, do I want to make a bigger druglike set?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from math import floor, ceil\n",
    "from pylab import rcParams\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import VarianceThreshold \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(series): \n",
    "    return pd.get_dummies(series.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('../../big datasets/drugml/x_train_res.csv')\n",
    "train_y = pd.read_csv('../../big datasets/drugml/y_train_res.csv')\n",
    "test_x = pd.read_csv('../../big datasets/drugml/x_test.csv')\n",
    "test_y = pd.read_csv('../../big datasets/drugml/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ellie\\Miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ellie\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ellie\\Miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ellie\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "xscaler = StandardScaler().fit(train_x)\n",
    "train_x = xscaler.transform(train_x)\n",
    "testscaler = StandardScaler().fit(test_x)\n",
    "test_x = testscaler.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(train_x)\n",
    "train_y = pd.DataFrame(train_y)\n",
    "x_test = pd.DataFrame(test_x)\n",
    "test_y = pd.DataFrame(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "train_y = train_y.drop(['Unnamed: 0'], axis=1)\n",
    "y_train = encode(train_y)\n",
    "#y_train = encode(train_y)\n",
    "#x_test = test_x.drop(['Unnamed: 0'], axis=1)\n",
    "test_y = test_y.drop(['Unnamed: 0'], axis=1)\n",
    "y_test = encode(test_y)\n",
    "#y_test = encode(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1626, 280)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "tf.set_random_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases, keep_prob):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'hidden' : [10, 20, 30, 40, 50],\n",
    "             'prob' : [0.2, 0.4, 0.6, 0.8],\n",
    "             'size': [32, 45, 60],\n",
    "             'rate' : [0.05, 0.01, 0.001, 0.001]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class_0</th>\n",
       "      <th>Class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class_0  Class_1\n",
       "0        0        1\n",
       "1        0        1\n",
       "2        0        1\n",
       "3        0        1\n",
       "4        0        1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.DataFrame(y_test)\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 13.583344702\n",
      "Epoch: 0002 cost= 5.395451939\n",
      "Epoch: 0003 cost= 2.228134196\n",
      "Optimization Finished!\n",
      "Accuracy: 0.800813\n",
      "[[ 42   0]\n",
      " [ 49 155]]\n"
     ]
    }
   ],
   "source": [
    " #### use meeeeee !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "n_hidden_1 = 40\n",
    "n_input = train_x.shape[1]\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "training_epochs = 3\n",
    "display_step = 1\n",
    "batch_size = 60\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        x_batches = np.array_split(x_train, total_batch)\n",
    "        y_batches = np.array_split(y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: 0.35\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    \n",
    "    confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "    print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.000000000\n",
      "Epoch: 0002 cost= 0.000000000\n",
      "Epoch: 0003 cost= 0.000000000\n",
      "Epoch: 0004 cost= 0.000000000\n",
      "Epoch: 0005 cost= 0.000000000\n",
      "Epoch: 0006 cost= 0.000000000\n",
      "Epoch: 0007 cost= 0.000000000\n",
      "Epoch: 0008 cost= 0.000000000\n",
      "Epoch: 0009 cost= 0.000000000\n",
      "Epoch: 0010 cost= 0.000000000\n",
      "Optimization Finished!\n",
      "Accuracy: 1.0\n",
      "[[246   0]\n",
      " [  0   0]]\n"
     ]
    }
   ],
   "source": [
    "#### use meeeeee \n",
    "#### use meeeeee\n",
    "n_hidden_1 = 60\n",
    "n_input = train_x.shape[1]\n",
    "n_classes = train_y.shape[1]\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "training_epochs = 10\n",
    "display_step = 1\n",
    "batch_size = 60\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        x_train, y_train = shuffle(x_train, y_train, random_state=12)\n",
    "        x_batches = np.array_split(x_train, total_batch)\n",
    "        y_batches = np.array_split(y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: 0.35\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    x_test, y_test = shuffle(x_test, y_test, random_state=12)\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    \n",
    "    confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "    print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = run_model(setup_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'hidden' : [10, 20, 30, 40, 50],\n",
    "             'prob' : [0.2, 0.4, 0.6, 0.8],\n",
    "             'size': [32, 45, 60],\n",
    "             'rate' : [0.05, 0.01, 0.001, 0.001]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid = [30, 35, 40, 45, 50, 55, 60, 65, 70]\n",
    "pro = [0.3, 0.35, 0.4]\n",
    "rat = [0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid = [30, 40, 50]\n",
    "pro = [0.3, 0.5, 0.7]\n",
    "rat = [0.1]\n",
    "siz = [60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for hidden in hid:\n",
    "    for prob in pro:\n",
    "        for rate in rat:\n",
    "                \n",
    "            mod = []\n",
    "                    \n",
    "            n_hidden_1 = hidden\n",
    "            n_input = train_x.shape[1]\n",
    "            n_classes = y_train.shape[1]\n",
    "            weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "            biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "            keep_prob = tf.placeholder(\"float\", None)\n",
    "\n",
    "            training_epochs = 20\n",
    "            display_step = 2\n",
    "            batch_size = 60\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, n_input])\n",
    "            y = tf.placeholder(\"float\", [None, n_classes])\n",
    "            predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=rate).minimize(cost)\n",
    "            \n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                for epoch in range(training_epochs):\n",
    "                    avg_cost = 0.0\n",
    "                    total_batch = int(len(x_train) / batch_size)\n",
    "                    x_train, y_train = shuffle(x_train, y_train, random_state=12)\n",
    "                    x_batches = np.array_split(x_train, total_batch)\n",
    "                    y_batches = np.array_split(y_train, total_batch)\n",
    "                for i in range(total_batch):\n",
    "                    batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "                    _, c = sess.run([optimizer, cost], \n",
    "                                feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: prob})\n",
    "                    avg_cost += c / total_batch\n",
    "                if epoch % display_step == 0:\n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost))\n",
    "                correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                mod.append(accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "                \n",
    "    \n",
    "                confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "                mod.append(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "                mod.append(hidden)\n",
    "                mod.append(prob)\n",
    "                mod.append(rate)\n",
    "                \n",
    "                acc.append(mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.68292683, array([[ 42,   0],\n",
       "         [ 78, 126]]), 30, 0.7, 0.1], [0.7479675, array([[ 42,   0],\n",
       "         [ 62, 142]]), 40, 0.5, 0.1], [0.7479675, array([[ 42,   0],\n",
       "         [ 62, 142]]), 50, 0.3, 0.1], [0.75609756, array([[ 41,   1],\n",
       "         [ 59, 145]]), 40, 0.7, 0.1], [0.7886179, array([[ 42,   0],\n",
       "         [ 52, 152]]), 50, 0.5, 0.1], [0.79268295, array([[ 42,   0],\n",
       "         [ 51, 153]]), 30, 0.5, 0.1], [0.796748, array([[ 41,   1],\n",
       "         [ 49, 155]]), 30, 0.3, 0.1], [0.796748, array([[ 41,   1],\n",
       "         [ 49, 155]]), 40, 0.3, 0.1], [0.8089431, array([[ 42,   0],\n",
       "         [ 47, 157]]), 50, 0.7, 0.1]]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(acc, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.800813, array([[ 41,   1], [ 48, 156]]), 30, 20, 0.1], \n",
    "[0.800813, array([[ 42,   0], [ 49, 155]]), 70, 40, 0.1], \n",
    "[0.8130081, array([[ 42,   0], [ 46, 158]]), 70, 60, 0.1], \n",
    "[0.81707317, array([[ 33,   9], [ 36, 168]]), 70, 40, 0.001], \n",
    "[0.8292683, array([[ 41,   1], [ 41, 163]]), 50, 20, 0.1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.7195122, array([[ 42,   0], [ 69, 135]]), 30, 60, 0.1], \n",
    "[0.7357724, array([[ 42,   0], [ 65, 139]]), 50, 45, 0.1], \n",
    "[0.7479675, array([[ 42,   0], [ 62, 142]]), 70, 32, 0.1], \n",
    "[0.7479675, array([[ 20,  22], [ 40, 164]]), 100, 32, 0.001], \n",
    "[0.7723577, array([[ 42,   0], [ 56, 148]]), 50, 32, 0.1], \n",
    "[0.7723577, array([[ 42,   0], [ 56, 148]]), 100, 32, 0.1], \n",
    "[0.7804878, array([[ 42,   0], [ 54, 150]]), 30, 45, 0.1], \n",
    "[0.7804878, array([[ 17,  25], [ 29, 175]]), 50, 60, 0.001], \n",
    "[0.7886179, array([[ 42,   0], [ 52, 152]]), 50, 60, 0.1], \n",
    "[0.7886179, array([[ 42,   0], [ 52, 152]]), 100, 45, 0.1], \n",
    "[0.7886179, array([[ 29,  13], [ 39, 165]]), 100, 45, 0.001], \n",
    "[0.7886179, array([[ 42,   0], [ 52, 152]]), 100, 60, 0.1], \n",
    "[0.796748, array([[ 41,   1], [ 49, 155]]), 30, 32, 0.1], \n",
    "[0.796748, array([[ 18,  24], [ 26, 178]]), 70, 45, 0.001], \n",
    "[0.8130081, array([[ 41,   1], [ 45, 159]]), 70, 45, 0.1], \n",
    "[0.8211382, array([[ 42,   0], [ 44, 160]]), 70, 60, 0.1], # explore same batch sizes, explore hidden less than 100\n",
    "[0.85365856, array([[ 32,  10], [ 26, 178]]), 70, 32, 0.001]] # explore 0.1 versus 0.01 versus 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.7706422, array([[110,  36],\n",
    "         [ 14,  58]]), 10, 0.7, 60, 0.1], \n",
    "[0.77522933, array([[117,  29],\n",
    "         [ 20,  52]]), 30, 0.5, 60, 0.1], \n",
    "[0.77522933, array([[127,  19],\n",
    "         [ 30,  42]]), 70, 0.7, 60, 0.1], \n",
    "[0.7844037, array([[131,  15],\n",
    "         [ 32,  40]]), 50, 0.3, 60, 0.1], \n",
    "[0.7844037, array([[133,  13],\n",
    "         [ 34,  38]]), 50, 0.7, 60, 0.1], # best 0.1 learning rate, dropout 0.5 or 0.3, best hidden unknown...\n",
    "[0.78899086, array([[129,  17],\n",
    "         [ 29,  43]]), 30, 0.7, 60, 0.1], \n",
    "[0.79816514, array([[113,  33],\n",
    "         [ 11,  61]]), 10, 0.5, 60, 0.1], \n",
    "[0.79816514, array([[138,   8],\n",
    "         [ 36,  36]]), 50, 0.5, 60, 0.1], \n",
    "[0.8027523, array([[119,  27],\n",
    "         [ 16,  56]]), 30, 0.3, 60, 0.1], \n",
    "[0.8027523, array([[137,   9],\n",
    "         [ 34,  38]]), 70, 0.3, 60, 0.1], \n",
    "[0.8348624, array([[128,  18],\n",
    "         [ 18,  54]]), 70, 0.5, 60, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " [0.83027524, array([[132,  14],\n",
    "         [ 23,  49]]), 50, 0.3, 60, 0.1]] # narrow in on 50 and 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.8211009, array([[126,  20],\n",
    "         [ 19,  53]]), 60, 0.35, 60, 0.1], winner \n",
    "hyperparameters: 60, .35, 60, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, Y = x_train, y_train\n",
    "\n",
    "# Create model for KerasClassifier\n",
    "def create_model(hparams1,\n",
    "                 hparams2,\n",
    "                 hparams3,\n",
    "                 hparams4):\n",
    "    \n",
    "    n_hidden_1 = hparams1\n",
    "    n_input = train_x.shape[1]\n",
    "    n_classes = train_y.shape[1]\n",
    "\n",
    "    weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "    training_epochs = 200\n",
    "    display_step = 90\n",
    "    batch_size = hparams3\n",
    "\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hparams4).minimize(cost)\n",
    "\n",
    "    return\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        x_batches = np.array_split(x_train, total_batch)\n",
    "        y_batches = np.array_split(y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                        feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: hparams2\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "            \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    \n",
    "    confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "    print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "hparams1 = randint(10, 100)\n",
    "hparams2 = randint(0,10)*0.1\n",
    "hparams3 = randint(30,60)\n",
    "hparams4 = [0.1, 0.05, 0.01, 0.001]\n",
    "\n",
    "# Prepare the Dict for the Search\n",
    "param_dist = dict(hparams1=hparams1, \n",
    "                  hparams2=hparams2, \n",
    "                  hparams3=hparams3, \n",
    "                  hparams4=hparams4)\n",
    "\n",
    "# Search in action!\n",
    "n_iter_search = 16 # Number of parameter settings that are sampled.\n",
    "random_search = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(X, Y)\n",
    "\n",
    "# Show the results\n",
    "print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "means = random_search.cv_results_['mean_test_score']\n",
    "stds = random_search.cv_results_['std_test_score']\n",
    "params = random_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "# Create model for KerasClassifier\n",
    "   \n",
    "n_hidden_1 = hparams1\n",
    "n_input = train_x.shape[1]\n",
    "n_classes = train_y.shape[1]\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "training_epochs = 200\n",
    "display_step = 90\n",
    "batch_size = hparams3\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=hparams4).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        x_batches = np.array_split(x_train, total_batch)\n",
    "        y_batches = np.array_split(y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                        feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: hparams2\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "            \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    \n",
    "    confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "    print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "hparams1 = randint(10, 100)\n",
    "hparams2 = randint(0,10)*0.1\n",
    "hparams3 = randint(30,60)\n",
    "hparams4 = [0.1, 0.05, 0.01, 0.001]\n",
    "\n",
    "# Prepare the Dict for the Search\n",
    "param_dist = dict(hparams1=hparams1, \n",
    "                  hparams2=hparams2, \n",
    "                  hparams3=hparams3, \n",
    "                  hparams4=hparams4)\n",
    "\n",
    "# Search in action!\n",
    "n_iter_search = 16 # Number of parameter settings that are sampled.\n",
    "random_search = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(X, Y)\n",
    "\n",
    "# Show the results\n",
    "print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "means = random_search.cv_results_['mean_test_score']\n",
    "stds = random_search.cv_results_['std_test_score']\n",
    "params = random_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_val, Y_val):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense({{choice([10, 20, 40, 104])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([10, 20, 40, 104])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    if conditional({{choice(['two', 'three'])}}) == 'three':\n",
    "        model.add(Dense({{choice([10, 20, 40, 104])}}))\n",
    "        model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "        \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    adam = keras.optimizers.Adam(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "    rmsprop = keras.optimizers.RMSprop(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "    sgd = keras.optimizers.SGD(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "   \n",
    "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
    "    if choiceval == 'adam':\n",
    "        optim = adam\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = rmsprop\n",
    "    else:\n",
    "        optim = sgd\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size={{choice([128,256,512])}},\n",
    "              nb_epoch=20,\n",
    "              verbose=2,\n",
    "              validation_data=(X_val, Y_val))\n",
    "    score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_train, Y_train, X_val, Y_val\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=30,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Neural Network-Copy1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to make this different... this needs to have a different train/test layout because the test data is not getting called properly. try https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428 this method here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "\n",
    "\n",
    "    confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "    print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " total_error = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "    unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y, predictions)))\n",
    "    R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "    print(R_squared.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn naive random oversampling (imbalanced data)\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.tensorflow.balanced_batch_generator.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* look for network architecture from paper that used the QSAR \n",
    "* lasso \n",
    "* fix for sparse data\n",
    "* find any columns that are uniform (or very low variation)\n",
    "* normalize \n",
    "* tensorboard\n",
    "* early stopping - ask rainie if i need help\n",
    "* put layer in after dropout\n",
    "* if oversampling, up the dropout (is there a ratio)\n",
    "* test set needs to be balanced but not oversampled \n",
    "* use their train/test split, then shuffle the data\n",
    "* combine the set up cells and do a for loop for the number of nodes (like 5-50 at 5 or 10 node increments)\n",
    "    * use on waffle?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create environment (homoganize)\n",
    "install everything i need into it\n",
    "* do the wget thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    MSE = tf.metrics.mean_squared_error(tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    print(\"MSE:\", MSE)\n",
    "    fn = tf.metrics.false_negatives(\n",
    "    tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    tn = tf.metrics.true_negatives(\n",
    "    tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    fp = tf.metrics.false_positives(\n",
    "    tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    tp = tf.metrics.true_positives(\n",
    "    tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    print(\"FN:\", fn, \"TN:\", tn, \"FP:\", fp, \"TP:\", tp)\n",
    "    total_error = tf.reduce_sum(tf.square(tf.subtract(tf.cast(y_test, tf.float32), tf.reduce_mean(tf.cast(y_test, tf.float32)))))\n",
    "    unexplained_error = tf.reduce_sum(tf.square(tf.subtract(tf.cast(y_test, tf.float32), tf.cast(predictions, tf.float32))))\n",
    "    R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "    print(R_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n",
    "...                    'num_wings': [2, 0, 0, 0],\n",
    "...                    'num_specimen_seen': [10, 2, 1, 8]},\n",
    "...                   index=['falcon', 'dog', 'spider', 'fish'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1 = df.iloc[:,1:3]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
