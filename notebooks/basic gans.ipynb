{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(series): \n",
    "    return pd.get_dummies(series.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('../../big datasets/drugml/x_train_res.csv')\n",
    "train_y = pd.read_csv('../../big datasets/drugml/y_train_res.csv')\n",
    "test_x = pd.read_csv('../../big datasets/drugml/x_test.csv')\n",
    "test_y = pd.read_csv('../../big datasets/drugml/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ellie\\Miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ellie\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ellie\\Miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ellie\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "xscaler = StandardScaler().fit(train_x)\n",
    "train_x = xscaler.transform(train_x)\n",
    "testscaler = StandardScaler().fit(test_x)\n",
    "test_x = testscaler.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(train_x)\n",
    "train_y = pd.DataFrame(train_y)\n",
    "x_test = pd.DataFrame(test_x)\n",
    "test_y = pd.DataFrame(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "train_y = train_y.drop(['Unnamed: 0'], axis=1)\n",
    "y_train = encode(train_y)\n",
    "#y_train = encode(train_y)\n",
    "#x_test = test_x.drop(['Unnamed: 0'], axis=1)\n",
    "test_y = test_y.drop(['Unnamed: 0'], axis=1)\n",
    "y_test = encode(test_y)\n",
    "#y_test = encode(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>Class-1</th>\n",
       "      <th>Class-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.730986</td>\n",
       "      <td>0.921982</td>\n",
       "      <td>-0.157973</td>\n",
       "      <td>-1.459674</td>\n",
       "      <td>0.334478</td>\n",
       "      <td>0.369859</td>\n",
       "      <td>0.428547</td>\n",
       "      <td>1.187530</td>\n",
       "      <td>-0.313013</td>\n",
       "      <td>-0.179779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877483</td>\n",
       "      <td>0.613616</td>\n",
       "      <td>-0.820161</td>\n",
       "      <td>-0.338000</td>\n",
       "      <td>0.806019</td>\n",
       "      <td>0.234209</td>\n",
       "      <td>1.225323</td>\n",
       "      <td>-1.425786</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.728855</td>\n",
       "      <td>0.855233</td>\n",
       "      <td>-0.157973</td>\n",
       "      <td>-1.165434</td>\n",
       "      <td>-0.087522</td>\n",
       "      <td>0.817984</td>\n",
       "      <td>1.128209</td>\n",
       "      <td>0.677041</td>\n",
       "      <td>2.486413</td>\n",
       "      <td>-0.179779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937737</td>\n",
       "      <td>1.266806</td>\n",
       "      <td>0.683059</td>\n",
       "      <td>-0.337961</td>\n",
       "      <td>0.963520</td>\n",
       "      <td>0.346804</td>\n",
       "      <td>1.350914</td>\n",
       "      <td>-0.382318</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.726725</td>\n",
       "      <td>-0.490926</td>\n",
       "      <td>-0.157973</td>\n",
       "      <td>-0.092644</td>\n",
       "      <td>-0.689763</td>\n",
       "      <td>0.821746</td>\n",
       "      <td>0.428547</td>\n",
       "      <td>0.166552</td>\n",
       "      <td>-0.313013</td>\n",
       "      <td>-0.179779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.617204</td>\n",
       "      <td>-0.217306</td>\n",
       "      <td>-0.337907</td>\n",
       "      <td>0.784516</td>\n",
       "      <td>0.449160</td>\n",
       "      <td>0.220684</td>\n",
       "      <td>-0.429355</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.724595</td>\n",
       "      <td>1.783538</td>\n",
       "      <td>-0.157973</td>\n",
       "      <td>-2.115771</td>\n",
       "      <td>1.673549</td>\n",
       "      <td>0.956781</td>\n",
       "      <td>2.527533</td>\n",
       "      <td>-0.854426</td>\n",
       "      <td>-0.313013</td>\n",
       "      <td>-0.179779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729878</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>-0.049837</td>\n",
       "      <td>-0.337926</td>\n",
       "      <td>1.128384</td>\n",
       "      <td>0.467271</td>\n",
       "      <td>0.824613</td>\n",
       "      <td>-0.956659</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.722464</td>\n",
       "      <td>1.576920</td>\n",
       "      <td>-0.157973</td>\n",
       "      <td>-0.512310</td>\n",
       "      <td>-0.629174</td>\n",
       "      <td>0.681700</td>\n",
       "      <td>0.428547</td>\n",
       "      <td>0.166552</td>\n",
       "      <td>-0.313013</td>\n",
       "      <td>-0.179779</td>\n",
       "      <td>...</td>\n",
       "      <td>1.639030</td>\n",
       "      <td>2.540884</td>\n",
       "      <td>1.382322</td>\n",
       "      <td>-0.338042</td>\n",
       "      <td>0.779403</td>\n",
       "      <td>0.436493</td>\n",
       "      <td>0.351273</td>\n",
       "      <td>-0.689293</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 282 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.730986  0.921982 -0.157973 -1.459674  0.334478  0.369859  0.428547   \n",
       "1 -1.728855  0.855233 -0.157973 -1.165434 -0.087522  0.817984  1.128209   \n",
       "2 -1.726725 -0.490926 -0.157973 -0.092644 -0.689763  0.821746  0.428547   \n",
       "3 -1.724595  1.783538 -0.157973 -2.115771  1.673549  0.956781  2.527533   \n",
       "4 -1.722464  1.576920 -0.157973 -0.512310 -0.629174  0.681700  0.428547   \n",
       "\n",
       "          7         8         9  ...       272       273       274       275  \\\n",
       "0  1.187530 -0.313013 -0.179779  ...  0.877483  0.613616 -0.820161 -0.338000   \n",
       "1  0.677041  2.486413 -0.179779  ...  0.937737  1.266806  0.683059 -0.337961   \n",
       "2  0.166552 -0.313013 -0.179779  ...  0.924800  0.617204 -0.217306 -0.337907   \n",
       "3 -0.854426 -0.313013 -0.179779  ...  0.729878  0.575758 -0.049837 -0.337926   \n",
       "4  0.166552 -0.313013 -0.179779  ...  1.639030  2.540884  1.382322 -0.338042   \n",
       "\n",
       "        276       277       278       279  Class-1  Class-2  \n",
       "0  0.806019  0.234209  1.225323 -1.425786        0        1  \n",
       "1  0.963520  0.346804  1.350914 -0.382318        0        1  \n",
       "2  0.784516  0.449160  0.220684 -0.429355        0        1  \n",
       "3  1.128384  0.467271  0.824613 -0.956659        0        1  \n",
       "4  0.779403  0.436493  0.351273 -0.689293        0        1  \n",
       "\n",
       "[5 rows x 282 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['Class-1'] = y_train.iloc[:,0]\n",
    "x_train['Class-2'] = y_train.iloc[:,1]\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GAN_171103'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-f311f68a0e15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mGAN_171103\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'GAN_171103'"
     ]
    }
   ],
   "source": [
    "from GAN_171103 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z,reuse=None):\n",
    "    with tf.variable_scope('gen', reuse=reuse):\n",
    "        hidden1=tf.layers.dense(inputs=z, units=60, activation=tf.nn.leaky_relu)\n",
    "        hidden2=tf.layers.dense(inputs=hidden1, units=60, activation=tf.nn.leaky_relu)\n",
    "        output=tf.layers.dense(inputs=hidden2, units=280, activation=tf.nn.tanh)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def discriminator(x, reuse=None):\n",
    "    with tf.variable_scope('dis', reuse=reuse):\n",
    "        hidden1=tf.layers.dense(inputs=x, units=60, activation=tf.nn.leaky_relu)\n",
    "        hidden2=tf.layers.dense(inputs=hidden1, units=60, activation=tf.nn.leaky_relu)\n",
    "        logits=tf.layers.dense(hidden2, units=280)\n",
    "        output=tf.sigmoid(logits)\n",
    "        \n",
    "        return output, logits\n",
    "    ### discriminator should be similar to what i have in the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "real_molecules=tf.placeholder(tf.float32, shape=[None,280])\n",
    "z = tf.placeholder(tf.float32, shape=[None,280])\n",
    "\n",
    "G = generator(z)\n",
    "D_output_real,D_logits_real = discriminator(real_molecules)\n",
    "D_output_fake,D_logits_fake = discriminator(G, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(logits_in, labels_in):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in, labels=labels_in))\n",
    "                          \n",
    "D_real_loss = loss_func(D_logits_real,tf.ones_like(D_logits_real)*0.9)\n",
    "D_fake_loss = loss_func(D_logits_fake, tf.zeros_like(D_logits_real))\n",
    "D_loss=D_real_loss + D_fake_loss\n",
    "                          \n",
    "G_loss = loss_func(D_logits_fake, tf.ones_like(D_logits_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "tvars=tf.trainable_variables()  #returns all variables created(the two variable scopes) and makes trainable true\n",
    "d_vars=[var for var in tvars if 'dis' in var.name]\n",
    "g_vars=[var for var in tvars if 'gen' in var.name]\n",
    "\n",
    "D_trainer=tf.train.AdamOptimizer(lr).minimize(D_loss,var_list=d_vars)\n",
    "G_trainer=tf.train.AdamOptimizer(lr).minimize(G_loss,var_list=g_vars)\n",
    "\n",
    "batch_size=100\n",
    "epochs=100\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-570fdf211234>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mbatch_mols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#reshape((batch_size,280))\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mbatch_mols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_mols\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mbatch_z\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m280\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        #num_batches=mnist.train.num_examples//batch_size\n",
    "        num_batches = int(len(x_train)/batch_size)\n",
    "        x_batches = np.array_split(x_train, num_batches)\n",
    "        y_batches = np.array_split(y_train, num_batches)\n",
    "        for i in range(num_batches):\n",
    "            #batch=mnist.train.next_batch(batch_size)\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            \n",
    "            batch_mols=batch_x[0] + batch_y[0] #reshape((batch_size,280))\n",
    "            batch_mols=batch_mols*2-1\n",
    "            batch_z=np.random.uniform(-1,1,size=(batch_size,280))\n",
    "            _=sess.run(D_trainer,feed_dict={real_molecules:batch_mols,z:batch_z})\n",
    "            _=sess.run(G_trainer,feed_dict={z:batch_z})\n",
    "            \n",
    "        print(\"on epoch{}\".format(epoch))\n",
    "        \n",
    "        sample_z=np.random.uniform(-1,1,size=(1,100))\n",
    "        gen_sample=sess.run(generator(z,reuse=True),feed_dict={z:sample_z})\n",
    "        \n",
    "        samples.append(gen_sample)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def generator(Z,hsize=[16, 16],reuse=False):\n",
    "    with tf.variable_scope(\"GAN/Generator\",reuse=reuse):\n",
    "        h1 = tf.layers.dense(Z,hsize[0],activation=tf.nn.leaky_relu)\n",
    "        h2 = tf.layers.dense(h1,hsize[1],activation=tf.nn.leaky_relu)\n",
    "        out = tf.layers.dense(h2,2)\n",
    "\n",
    "    return out\n",
    "\n",
    "def discriminator(X,hsize=[16, 16],reuse=False):\n",
    "    with tf.variable_scope(\"GAN/Discriminator\",reuse=reuse):\n",
    "        h1 = tf.layers.dense(X,hsize[0],activation=tf.nn.leaky_relu)\n",
    "        h2 = tf.layers.dense(h1,hsize[1],activation=tf.nn.leaky_relu)\n",
    "        h3 = tf.layers.dense(h2,2)\n",
    "        out = tf.layers.dense(h3,1)\n",
    "\n",
    "    return out, h3\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,2])\n",
    "Z = tf.placeholder(tf.float32,[None,2])\n",
    "\n",
    "G_sample = generator(Z)\n",
    "r_logits, r_rep = discriminator(X)\n",
    "f_logits, g_rep = discriminator(G_sample,reuse=True)\n",
    "\n",
    "disc_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=r_logits,labels=tf.ones_like(r_logits)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,labels=tf.zeros_like(f_logits)))\n",
    "gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,labels=tf.ones_like(f_logits)))\n",
    "\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Generator\")\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Discriminator\")\n",
    "\n",
    "gen_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(gen_loss,var_list = gen_vars) # G Train step\n",
    "disc_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(disc_loss,var_list = disc_vars) # D Train step\n",
    "\n",
    "\n",
    "\n",
    "# sess = tf.Session(config=config)\n",
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "batch_size = 256\n",
    "nd_steps = 10\n",
    "ng_steps = 10\n",
    "\n",
    "x_plot = sample_data(n=batch_size)\n",
    "\n",
    "f = open('loss_logs.csv','w')\n",
    "f.write('Iteration,Discriminator Loss,Generator Loss\\n')\n",
    "\n",
    "for i in range(10001):\n",
    "    X_batch = sample_data(n=batch_size)\n",
    "    Z_batch = sample_Z(batch_size, 2)\n",
    "\n",
    "    for _ in range(nd_steps):\n",
    "        _, dloss = sess.run([disc_step, disc_loss], feed_dict={X: X_batch, Z: Z_batch})\n",
    "    rrep_dstep, grep_dstep = sess.run([r_rep, g_rep], feed_dict={X: X_batch, Z: Z_batch})\n",
    "\n",
    "    for _ in range(ng_steps):\n",
    "        _, gloss = sess.run([gen_step, gen_loss], feed_dict={Z: Z_batch})\n",
    "\n",
    "    rrep_gstep, grep_gstep = sess.run([r_rep, g_rep], feed_dict={X: X_batch, Z: Z_batch})\n",
    "\n",
    "    print \"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\"%(i,dloss,gloss)\n",
    "    if i%10 == 0:\n",
    "        f.write(\"%d,%f,%f\\n\"%(i,dloss,gloss))\n",
    "        \n",
    "        #### training data\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def get_y(x):\n",
    "    return 10 + x*x\n",
    "\n",
    "\n",
    "def sample_data(n=10000, scale=100):\n",
    "    data = []\n",
    "\n",
    "    x = scale*(np.random.random_sample((n,))-0.5)\n",
    "\n",
    "    for i in range(n):\n",
    "        yi = get_y(x[i])\n",
    "        data.append([x[i], yi])\n",
    "\n",
    "    return np.array(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
