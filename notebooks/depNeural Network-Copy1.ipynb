{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from math import floor, ceil\n",
    "from pylab import rcParams\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import VarianceThreshold \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "biodeg = pd.read_csv('../../big datasets/featurized_biodeg.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Class</th>\n",
       "      <th>Status</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>AMR</th>\n",
       "      <th>apol</th>\n",
       "      <th>naAromAtom</th>\n",
       "      <th>...</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WTPT-1</th>\n",
       "      <th>WTPT-2</th>\n",
       "      <th>WTPT-3</th>\n",
       "      <th>WTPT-4</th>\n",
       "      <th>WTPT-5</th>\n",
       "      <th>WPATH</th>\n",
       "      <th>WPOL</th>\n",
       "      <th>XLogP</th>\n",
       "      <th>Zagreb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CCCCCCCCCCC</td>\n",
       "      <td>RB</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.4894</td>\n",
       "      <td>6.197112</td>\n",
       "      <td>38.0018</td>\n",
       "      <td>35.363032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.462509</td>\n",
       "      <td>20.828762</td>\n",
       "      <td>1.893524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220</td>\n",
       "      <td>8</td>\n",
       "      <td>6.599</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CCC1CO1</td>\n",
       "      <td>RB</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4296</td>\n",
       "      <td>0.184556</td>\n",
       "      <td>18.5395</td>\n",
       "      <td>13.176344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.542886</td>\n",
       "      <td>9.148995</td>\n",
       "      <td>1.829799</td>\n",
       "      <td>2.705816</td>\n",
       "      <td>2.705816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.628</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CCOCCOCCOCCO</td>\n",
       "      <td>RB</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5003</td>\n",
       "      <td>0.250300</td>\n",
       "      <td>45.6856</td>\n",
       "      <td>29.290274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.937350</td>\n",
       "      <td>22.828595</td>\n",
       "      <td>1.902383</td>\n",
       "      <td>11.163248</td>\n",
       "      <td>11.163248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>OC=O</td>\n",
       "      <td>RB</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.3087</td>\n",
       "      <td>0.095296</td>\n",
       "      <td>8.5717</td>\n",
       "      <td>4.697586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.201096</td>\n",
       "      <td>4.914214</td>\n",
       "      <td>1.638071</td>\n",
       "      <td>4.414214</td>\n",
       "      <td>4.414214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CCCCOCCOCCOC(C)=O</td>\n",
       "      <td>RB</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8561</td>\n",
       "      <td>0.732907</td>\n",
       "      <td>50.2372</td>\n",
       "      <td>34.143860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.004005</td>\n",
       "      <td>26.661622</td>\n",
       "      <td>1.904402</td>\n",
       "      <td>11.202922</td>\n",
       "      <td>11.202922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>444</td>\n",
       "      <td>11</td>\n",
       "      <td>1.211</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1448 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             SMILES Class Status  nAcid   ALogP    ALogp2  \\\n",
       "0           0        CCCCCCCCCCC    RB  Train    0.0 -2.4894  6.197112   \n",
       "1           1            CCC1CO1    RB  Train    0.0 -0.4296  0.184556   \n",
       "2           2       CCOCCOCCOCCO    RB  Train    0.0 -0.5003  0.250300   \n",
       "3           3               OC=O    RB  Train    1.0 -0.3087  0.095296   \n",
       "4           4  CCCCOCCOCCOC(C)=O    RB  Train    0.0 -0.8561  0.732907   \n",
       "\n",
       "       AMR       apol  naAromAtom  ...       AMW     WTPT-1    WTPT-2  \\\n",
       "0  38.0018  35.363032         0.0  ...  4.462509  20.828762  1.893524   \n",
       "1  18.5395  13.176344         0.0  ...  5.542886   9.148995  1.829799   \n",
       "2  45.6856  29.290274         0.0  ...  5.937350  22.828595  1.902383   \n",
       "3   8.5717   4.697586         0.0  ...  9.201096   4.914214  1.638071   \n",
       "4  50.2372  34.143860         0.0  ...  6.004005  26.661622  1.904402   \n",
       "\n",
       "      WTPT-3     WTPT-4  WTPT-5  WPATH  WPOL  XLogP  Zagreb  \n",
       "0   0.000000   0.000000     0.0    220     8  6.599      38  \n",
       "1   2.705816   2.705816     0.0     17     2  0.628      22  \n",
       "2  11.163248  11.163248     0.0    286     9 -0.646      42  \n",
       "3   4.414214   4.414214     0.0      4     0 -0.316       6  \n",
       "4  11.202922  11.202922     0.0    444    11  1.211      52  \n",
       "\n",
       "[5 rows x 1448 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biodeg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(series): \n",
    "    return pd.get_dummies(series.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nAcid</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>AMR</th>\n",
       "      <th>apol</th>\n",
       "      <th>naAromAtom</th>\n",
       "      <th>nAromBond</th>\n",
       "      <th>nAtom</th>\n",
       "      <th>nHeavyAtom</th>\n",
       "      <th>nH</th>\n",
       "      <th>...</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WTPT-1</th>\n",
       "      <th>WTPT-2</th>\n",
       "      <th>WTPT-3</th>\n",
       "      <th>WTPT-4</th>\n",
       "      <th>WTPT-5</th>\n",
       "      <th>WPATH</th>\n",
       "      <th>WPOL</th>\n",
       "      <th>XLogP</th>\n",
       "      <th>Zagreb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.4894</td>\n",
       "      <td>6.197112</td>\n",
       "      <td>38.0018</td>\n",
       "      <td>35.363032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.462509</td>\n",
       "      <td>20.828762</td>\n",
       "      <td>1.893524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.599</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4296</td>\n",
       "      <td>0.184556</td>\n",
       "      <td>18.5395</td>\n",
       "      <td>13.176344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.542886</td>\n",
       "      <td>9.148995</td>\n",
       "      <td>1.829799</td>\n",
       "      <td>2.705816</td>\n",
       "      <td>2.705816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.628</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5003</td>\n",
       "      <td>0.250300</td>\n",
       "      <td>45.6856</td>\n",
       "      <td>29.290274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.937350</td>\n",
       "      <td>22.828595</td>\n",
       "      <td>1.902383</td>\n",
       "      <td>11.163248</td>\n",
       "      <td>11.163248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>286.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.3087</td>\n",
       "      <td>0.095296</td>\n",
       "      <td>8.5717</td>\n",
       "      <td>4.697586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.201096</td>\n",
       "      <td>4.914214</td>\n",
       "      <td>1.638071</td>\n",
       "      <td>4.414214</td>\n",
       "      <td>4.414214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8561</td>\n",
       "      <td>0.732907</td>\n",
       "      <td>50.2372</td>\n",
       "      <td>34.143860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.004005</td>\n",
       "      <td>26.661622</td>\n",
       "      <td>1.904402</td>\n",
       "      <td>11.202922</td>\n",
       "      <td>11.202922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>444.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.211</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>43.7936</td>\n",
       "      <td>27.956688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.289459</td>\n",
       "      <td>22.661828</td>\n",
       "      <td>1.888486</td>\n",
       "      <td>11.077237</td>\n",
       "      <td>11.077237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.284</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.4392</td>\n",
       "      <td>0.192897</td>\n",
       "      <td>16.6161</td>\n",
       "      <td>9.733172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.665346</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0933</td>\n",
       "      <td>1.195305</td>\n",
       "      <td>52.1502</td>\n",
       "      <td>38.727032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.267832</td>\n",
       "      <td>26.491096</td>\n",
       "      <td>1.892221</td>\n",
       "      <td>5.447103</td>\n",
       "      <td>5.447103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>372.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.417</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2962</td>\n",
       "      <td>5.272534</td>\n",
       "      <td>39.5198</td>\n",
       "      <td>20.747930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.893236</td>\n",
       "      <td>15.532958</td>\n",
       "      <td>1.941620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.866</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1813</td>\n",
       "      <td>1.395470</td>\n",
       "      <td>40.1727</td>\n",
       "      <td>19.961965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.824815</td>\n",
       "      <td>23.243258</td>\n",
       "      <td>1.936938</td>\n",
       "      <td>12.697752</td>\n",
       "      <td>9.666664</td>\n",
       "      <td>3.031088</td>\n",
       "      <td>188.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.777</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0310</td>\n",
       "      <td>1.062961</td>\n",
       "      <td>45.5986</td>\n",
       "      <td>21.444758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.590399</td>\n",
       "      <td>22.509724</td>\n",
       "      <td>2.046339</td>\n",
       "      <td>5.588995</td>\n",
       "      <td>5.588995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>144.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.022</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.7614</td>\n",
       "      <td>0.579730</td>\n",
       "      <td>20.5322</td>\n",
       "      <td>16.801516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.240818</td>\n",
       "      <td>8.849874</td>\n",
       "      <td>1.769975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.185</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5918</td>\n",
       "      <td>0.350227</td>\n",
       "      <td>27.5299</td>\n",
       "      <td>19.895102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.305478</td>\n",
       "      <td>10.679177</td>\n",
       "      <td>1.779863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.693</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0494</td>\n",
       "      <td>1.101240</td>\n",
       "      <td>23.4438</td>\n",
       "      <td>19.895102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.305478</td>\n",
       "      <td>10.839150</td>\n",
       "      <td>1.806525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.754</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.9134</td>\n",
       "      <td>3.661100</td>\n",
       "      <td>32.1786</td>\n",
       "      <td>29.175860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.419190</td>\n",
       "      <td>16.829768</td>\n",
       "      <td>1.869974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.461</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0728</td>\n",
       "      <td>4.296500</td>\n",
       "      <td>45.3425</td>\n",
       "      <td>39.969825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.372749</td>\n",
       "      <td>24.828511</td>\n",
       "      <td>1.909885</td>\n",
       "      <td>2.414011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>364.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.877</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0803</td>\n",
       "      <td>9.488248</td>\n",
       "      <td>59.9724</td>\n",
       "      <td>53.214169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.083531</td>\n",
       "      <td>32.828432</td>\n",
       "      <td>1.931084</td>\n",
       "      <td>2.414201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>816.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.491</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.5510</td>\n",
       "      <td>2.405601</td>\n",
       "      <td>23.5678</td>\n",
       "      <td>18.568309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.584463</td>\n",
       "      <td>10.839150</td>\n",
       "      <td>1.806525</td>\n",
       "      <td>2.388325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.388325</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.353</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>19.9881</td>\n",
       "      <td>12.381137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.544115</td>\n",
       "      <td>6.732051</td>\n",
       "      <td>1.683013</td>\n",
       "      <td>2.732051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.732051</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3580</td>\n",
       "      <td>0.128164</td>\n",
       "      <td>8.2613</td>\n",
       "      <td>5.229172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.337702</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.2614</td>\n",
       "      <td>1.591130</td>\n",
       "      <td>21.9103</td>\n",
       "      <td>17.603516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.893823</td>\n",
       "      <td>10.839150</td>\n",
       "      <td>1.806525</td>\n",
       "      <td>2.388325</td>\n",
       "      <td>2.388325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.420</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1416</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>15.6279</td>\n",
       "      <td>10.082758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.804186</td>\n",
       "      <td>7.175869</td>\n",
       "      <td>1.793967</td>\n",
       "      <td>2.632592</td>\n",
       "      <td>2.632592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8974</td>\n",
       "      <td>0.805327</td>\n",
       "      <td>14.5536</td>\n",
       "      <td>9.124758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.203678</td>\n",
       "      <td>6.871320</td>\n",
       "      <td>1.717830</td>\n",
       "      <td>4.621320</td>\n",
       "      <td>4.621320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.208</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9081</td>\n",
       "      <td>0.824646</td>\n",
       "      <td>19.1380</td>\n",
       "      <td>12.218344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.850187</td>\n",
       "      <td>8.696802</td>\n",
       "      <td>1.739360</td>\n",
       "      <td>4.642168</td>\n",
       "      <td>4.642168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.1961</td>\n",
       "      <td>1.430655</td>\n",
       "      <td>22.0496</td>\n",
       "      <td>15.311930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.629255</td>\n",
       "      <td>10.675851</td>\n",
       "      <td>1.779308</td>\n",
       "      <td>4.745721</td>\n",
       "      <td>4.745721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3162</td>\n",
       "      <td>0.099982</td>\n",
       "      <td>15.0410</td>\n",
       "      <td>8.749172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.003277</td>\n",
       "      <td>6.871320</td>\n",
       "      <td>1.717830</td>\n",
       "      <td>2.310660</td>\n",
       "      <td>2.310660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4996</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>23.8892</td>\n",
       "      <td>15.311930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.629255</td>\n",
       "      <td>10.679177</td>\n",
       "      <td>1.779863</td>\n",
       "      <td>4.994504</td>\n",
       "      <td>4.994504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.1043</td>\n",
       "      <td>1.219478</td>\n",
       "      <td>30.0422</td>\n",
       "      <td>21.499102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.368154</td>\n",
       "      <td>14.831108</td>\n",
       "      <td>1.853888</td>\n",
       "      <td>5.297907</td>\n",
       "      <td>5.297907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.661</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1063</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>16.7418</td>\n",
       "      <td>10.082758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.804186</td>\n",
       "      <td>6.871320</td>\n",
       "      <td>1.717830</td>\n",
       "      <td>2.310660</td>\n",
       "      <td>2.310660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.587</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4893</td>\n",
       "      <td>6.196614</td>\n",
       "      <td>50.2393</td>\n",
       "      <td>29.070688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.634078</td>\n",
       "      <td>20.486117</td>\n",
       "      <td>1.862374</td>\n",
       "      <td>2.404409</td>\n",
       "      <td>2.404409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>194.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.701</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.3636</td>\n",
       "      <td>1.859405</td>\n",
       "      <td>53.3427</td>\n",
       "      <td>27.645516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.203595</td>\n",
       "      <td>25.261329</td>\n",
       "      <td>1.943179</td>\n",
       "      <td>10.649353</td>\n",
       "      <td>5.169871</td>\n",
       "      <td>5.479482</td>\n",
       "      <td>260.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.143</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4753</td>\n",
       "      <td>20.028310</td>\n",
       "      <td>107.3913</td>\n",
       "      <td>42.889172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.718967</td>\n",
       "      <td>37.688465</td>\n",
       "      <td>1.983603</td>\n",
       "      <td>18.233900</td>\n",
       "      <td>3.108668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>696.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6.259</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2031</td>\n",
       "      <td>0.041250</td>\n",
       "      <td>45.2794</td>\n",
       "      <td>22.785137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.553166</td>\n",
       "      <td>21.405281</td>\n",
       "      <td>1.945935</td>\n",
       "      <td>7.599746</td>\n",
       "      <td>4.825813</td>\n",
       "      <td>2.773933</td>\n",
       "      <td>162.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.320</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>0.181646</td>\n",
       "      <td>60.9151</td>\n",
       "      <td>32.867895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.970173</td>\n",
       "      <td>29.259042</td>\n",
       "      <td>1.950603</td>\n",
       "      <td>11.139799</td>\n",
       "      <td>8.441999</td>\n",
       "      <td>2.697800</td>\n",
       "      <td>384.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.689</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4638</td>\n",
       "      <td>11.997910</td>\n",
       "      <td>97.4047</td>\n",
       "      <td>45.802309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.484094</td>\n",
       "      <td>39.888535</td>\n",
       "      <td>1.994427</td>\n",
       "      <td>17.231094</td>\n",
       "      <td>5.984388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>798.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.763</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6382</td>\n",
       "      <td>6.960099</td>\n",
       "      <td>77.7322</td>\n",
       "      <td>40.162274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.180613</td>\n",
       "      <td>32.361629</td>\n",
       "      <td>2.022602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>416.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.630</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4042</td>\n",
       "      <td>5.780178</td>\n",
       "      <td>81.1127</td>\n",
       "      <td>46.349446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.954304</td>\n",
       "      <td>37.385806</td>\n",
       "      <td>2.076989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>657.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.614</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0901</td>\n",
       "      <td>1.188318</td>\n",
       "      <td>73.0664</td>\n",
       "      <td>35.047516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.864781</td>\n",
       "      <td>34.131858</td>\n",
       "      <td>2.007756</td>\n",
       "      <td>7.903389</td>\n",
       "      <td>7.903389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>528.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.221</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2372</td>\n",
       "      <td>0.056264</td>\n",
       "      <td>62.9497</td>\n",
       "      <td>30.007137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.292930</td>\n",
       "      <td>29.988912</td>\n",
       "      <td>1.999261</td>\n",
       "      <td>12.983253</td>\n",
       "      <td>7.158431</td>\n",
       "      <td>2.552534</td>\n",
       "      <td>322.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.091</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.8003</td>\n",
       "      <td>0.640480</td>\n",
       "      <td>64.5552</td>\n",
       "      <td>30.809137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.561009</td>\n",
       "      <td>31.835518</td>\n",
       "      <td>1.989720</td>\n",
       "      <td>15.521978</td>\n",
       "      <td>9.715200</td>\n",
       "      <td>2.540473</td>\n",
       "      <td>406.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.8003</td>\n",
       "      <td>0.640480</td>\n",
       "      <td>64.5552</td>\n",
       "      <td>30.809137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.561009</td>\n",
       "      <td>31.835601</td>\n",
       "      <td>1.989725</td>\n",
       "      <td>15.520819</td>\n",
       "      <td>9.714803</td>\n",
       "      <td>2.539611</td>\n",
       "      <td>409.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1006</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>71.2394</td>\n",
       "      <td>33.557930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.787601</td>\n",
       "      <td>35.812338</td>\n",
       "      <td>1.989574</td>\n",
       "      <td>12.635009</td>\n",
       "      <td>12.635009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>604.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.963</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9780</td>\n",
       "      <td>0.956484</td>\n",
       "      <td>23.8892</td>\n",
       "      <td>16.878344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.001633</td>\n",
       "      <td>13.370400</td>\n",
       "      <td>1.910057</td>\n",
       "      <td>8.151650</td>\n",
       "      <td>4.825825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.466</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.111089</td>\n",
       "      <td>72.6538</td>\n",
       "      <td>41.548688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.002288</td>\n",
       "      <td>35.365803</td>\n",
       "      <td>2.080341</td>\n",
       "      <td>12.501579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.243204</td>\n",
       "      <td>573.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.536</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0152</td>\n",
       "      <td>1.030631</td>\n",
       "      <td>25.0514</td>\n",
       "      <td>13.511172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.335606</td>\n",
       "      <td>15.532829</td>\n",
       "      <td>1.941604</td>\n",
       "      <td>10.860943</td>\n",
       "      <td>4.990326</td>\n",
       "      <td>5.870617</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.190</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0910</td>\n",
       "      <td>4.372281</td>\n",
       "      <td>94.7973</td>\n",
       "      <td>57.815790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.846012</td>\n",
       "      <td>53.066049</td>\n",
       "      <td>1.965409</td>\n",
       "      <td>36.266241</td>\n",
       "      <td>16.492335</td>\n",
       "      <td>19.773906</td>\n",
       "      <td>1818.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6021</td>\n",
       "      <td>2.566724</td>\n",
       "      <td>45.9862</td>\n",
       "      <td>23.467930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.527818</td>\n",
       "      <td>17.382017</td>\n",
       "      <td>1.931335</td>\n",
       "      <td>11.754136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.326638</td>\n",
       "      <td>84.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.779</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5109</td>\n",
       "      <td>2.282819</td>\n",
       "      <td>87.2632</td>\n",
       "      <td>41.292309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.972565</td>\n",
       "      <td>41.097444</td>\n",
       "      <td>2.054872</td>\n",
       "      <td>8.201207</td>\n",
       "      <td>5.143169</td>\n",
       "      <td>3.058038</td>\n",
       "      <td>833.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.736</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.678976</td>\n",
       "      <td>207.2020</td>\n",
       "      <td>106.433376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.489576</td>\n",
       "      <td>108.041662</td>\n",
       "      <td>2.000772</td>\n",
       "      <td>49.803260</td>\n",
       "      <td>21.122418</td>\n",
       "      <td>18.451260</td>\n",
       "      <td>15287.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>7.164</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2605</td>\n",
       "      <td>0.067860</td>\n",
       "      <td>20.7397</td>\n",
       "      <td>10.509172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.558468</td>\n",
       "      <td>9.687500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.709</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4758</td>\n",
       "      <td>0.226386</td>\n",
       "      <td>59.9132</td>\n",
       "      <td>30.007137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.292930</td>\n",
       "      <td>30.118295</td>\n",
       "      <td>2.007886</td>\n",
       "      <td>14.539285</td>\n",
       "      <td>8.001878</td>\n",
       "      <td>3.051381</td>\n",
       "      <td>351.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.850</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6753</td>\n",
       "      <td>2.806630</td>\n",
       "      <td>58.4719</td>\n",
       "      <td>29.017137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.818227</td>\n",
       "      <td>26.294966</td>\n",
       "      <td>2.022690</td>\n",
       "      <td>14.916781</td>\n",
       "      <td>8.851783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>230.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.151</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0349</td>\n",
       "      <td>4.140818</td>\n",
       "      <td>95.5375</td>\n",
       "      <td>52.467860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.002942</td>\n",
       "      <td>54.270431</td>\n",
       "      <td>2.010016</td>\n",
       "      <td>27.826057</td>\n",
       "      <td>14.963556</td>\n",
       "      <td>12.862501</td>\n",
       "      <td>1698.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>-2.049</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2676</td>\n",
       "      <td>0.071610</td>\n",
       "      <td>62.7258</td>\n",
       "      <td>34.471895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.534219</td>\n",
       "      <td>33.243252</td>\n",
       "      <td>1.955485</td>\n",
       "      <td>15.993278</td>\n",
       "      <td>13.613177</td>\n",
       "      <td>2.380101</td>\n",
       "      <td>606.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3683</td>\n",
       "      <td>1.872245</td>\n",
       "      <td>26.5507</td>\n",
       "      <td>14.510758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.175831</td>\n",
       "      <td>8.849874</td>\n",
       "      <td>1.769975</td>\n",
       "      <td>4.724874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.436</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2415</td>\n",
       "      <td>1.541322</td>\n",
       "      <td>18.5896</td>\n",
       "      <td>9.903965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.494683</td>\n",
       "      <td>4.914214</td>\n",
       "      <td>1.638071</td>\n",
       "      <td>2.207107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.736</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.7781</td>\n",
       "      <td>60.498840</td>\n",
       "      <td>95.6897</td>\n",
       "      <td>80.824857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.591950</td>\n",
       "      <td>59.942890</td>\n",
       "      <td>1.933642</td>\n",
       "      <td>17.740711</td>\n",
       "      <td>14.775250</td>\n",
       "      <td>2.965461</td>\n",
       "      <td>4310.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.367</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3354</td>\n",
       "      <td>0.112493</td>\n",
       "      <td>32.4747</td>\n",
       "      <td>21.031930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.103711</td>\n",
       "      <td>19.169390</td>\n",
       "      <td>1.916939</td>\n",
       "      <td>10.899136</td>\n",
       "      <td>5.002370</td>\n",
       "      <td>5.896766</td>\n",
       "      <td>113.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.292</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7280</td>\n",
       "      <td>2.985984</td>\n",
       "      <td>43.3039</td>\n",
       "      <td>22.785137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.553166</td>\n",
       "      <td>21.406303</td>\n",
       "      <td>1.946028</td>\n",
       "      <td>7.858999</td>\n",
       "      <td>4.830427</td>\n",
       "      <td>3.028572</td>\n",
       "      <td>150.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.889</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1026</td>\n",
       "      <td>37.241727</td>\n",
       "      <td>106.6334</td>\n",
       "      <td>42.920000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.440388</td>\n",
       "      <td>43.029507</td>\n",
       "      <td>1.955887</td>\n",
       "      <td>25.164391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>907.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.156</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>837 rows Ã— 1423 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     nAcid   ALogP     ALogp2       AMR        apol  naAromAtom  nAromBond  \\\n",
       "0      0.0 -2.4894   6.197112   38.0018   35.363032         0.0        0.0   \n",
       "1      0.0 -0.4296   0.184556   18.5395   13.176344         0.0        0.0   \n",
       "2      0.0 -0.5003   0.250300   45.6856   29.290274         0.0        0.0   \n",
       "3      1.0 -0.3087   0.095296    8.5717    4.697586         0.0        0.0   \n",
       "4      0.0 -0.8561   0.732907   50.2372   34.143860         0.0        0.0   \n",
       "5      0.0  0.0097   0.000094   43.7936   27.956688         0.0        0.0   \n",
       "6      1.0 -0.4392   0.192897   16.6161    9.733172         0.0        0.0   \n",
       "7      0.0 -1.0933   1.195305   52.1502   38.727032         0.0        0.0   \n",
       "8      0.0  2.2962   5.272534   39.5198   20.747930         0.0        0.0   \n",
       "9      1.0  1.1813   1.395470   40.1727   19.961965         0.0        0.0   \n",
       "10     0.0  1.0310   1.062961   45.5986   21.444758         0.0        0.0   \n",
       "11     0.0 -0.7614   0.579730   20.5322   16.801516         0.0        0.0   \n",
       "12     0.0  0.5918   0.350227   27.5299   19.895102         0.0        0.0   \n",
       "13     0.0 -1.0494   1.101240   23.4438   19.895102         0.0        0.0   \n",
       "14     0.0 -1.9134   3.661100   32.1786   29.175860         0.0        0.0   \n",
       "15     0.0 -2.0728   4.296500   45.3425   39.969825         0.0        0.0   \n",
       "16     0.0 -3.0803   9.488248   59.9724   53.214169         0.0        0.0   \n",
       "17     0.0 -1.5510   2.405601   23.5678   18.568309         0.0        0.0   \n",
       "18     0.0  0.3201   0.102464   19.9881   12.381137         0.0        0.0   \n",
       "19     0.0 -0.3580   0.128164    8.2613    5.229172         0.0        0.0   \n",
       "20     0.0 -1.2614   1.591130   21.9103   17.603516         0.0        0.0   \n",
       "21     0.0 -0.1416   0.020051   15.6279   10.082758         0.0        0.0   \n",
       "22     0.0 -0.8974   0.805327   14.5536    9.124758         0.0        0.0   \n",
       "23     0.0 -0.9081   0.824646   19.1380   12.218344         0.0        0.0   \n",
       "24     0.0 -1.1961   1.430655   22.0496   15.311930         0.0        0.0   \n",
       "25     0.0  0.3162   0.099982   15.0410    8.749172         0.0        0.0   \n",
       "26     0.0 -0.4996   0.249600   23.8892   15.311930         0.0        0.0   \n",
       "27     0.0 -1.1043   1.219478   30.0422   21.499102         0.0        0.0   \n",
       "28     0.0 -0.1063   0.011300   16.7418   10.082758         0.0        0.0   \n",
       "29     0.0  2.4893   6.196614   50.2393   29.070688         0.0        0.0   \n",
       "..     ...     ...        ...       ...         ...         ...        ...   \n",
       "807    0.0 -1.3636   1.859405   53.3427   27.645516         0.0        0.0   \n",
       "808    0.0  4.4753  20.028310  107.3913   42.889172         0.0        0.0   \n",
       "809    1.0  0.2031   0.041250   45.2794   22.785137         0.0        0.0   \n",
       "810    0.0  0.4262   0.181646   60.9151   32.867895         0.0        0.0   \n",
       "811    0.0  3.4638  11.997910   97.4047   45.802309         0.0        0.0   \n",
       "812    0.0  2.6382   6.960099   77.7322   40.162274         0.0        0.0   \n",
       "813    0.0  2.4042   5.780178   81.1127   46.349446         0.0        0.0   \n",
       "814    0.0  1.0901   1.188318   73.0664   35.047516         0.0        0.0   \n",
       "815    1.0 -0.2372   0.056264   62.9497   30.007137         0.0        0.0   \n",
       "816    1.0 -0.8003   0.640480   64.5552   30.809137         0.0        0.0   \n",
       "817    1.0 -0.8003   0.640480   64.5552   30.809137         0.0        0.0   \n",
       "818    0.0 -0.1006   0.010120   71.2394   33.557930         0.0        0.0   \n",
       "819    0.0 -0.9780   0.956484   23.8892   16.878344         0.0        0.0   \n",
       "820    0.0  0.3333   0.111089   72.6538   41.548688         0.0        0.0   \n",
       "821    0.0 -1.0152   1.030631   25.0514   13.511172         0.0        0.0   \n",
       "822    0.0 -2.0910   4.372281   94.7973   57.815790         0.0        0.0   \n",
       "823    0.0  1.6021   2.566724   45.9862   23.467930         0.0        0.0   \n",
       "824    0.0  1.5109   2.282819   87.2632   41.292309         0.0        0.0   \n",
       "825    0.0  0.8240   0.678976  207.2020  106.433376         0.0        0.0   \n",
       "826    0.0 -0.2605   0.067860   20.7397   10.509172         0.0        0.0   \n",
       "827    0.0  0.4758   0.226386   59.9132   30.007137         0.0        0.0   \n",
       "828    0.0  1.6753   2.806630   58.4719   29.017137         0.0        0.0   \n",
       "829    0.0 -2.0349   4.140818   95.5375   52.467860         0.0        0.0   \n",
       "830    0.0 -0.2676   0.071610   62.7258   34.471895         0.0        0.0   \n",
       "831    0.0  1.3683   1.872245   26.5507   14.510758         0.0        0.0   \n",
       "832    0.0  1.2415   1.541322   18.5896    9.903965         0.0        0.0   \n",
       "833    0.0 -7.7781  60.498840   95.6897   80.824857         0.0        0.0   \n",
       "834    0.0 -0.3354   0.112493   32.4747   21.031930         0.0        0.0   \n",
       "835    0.0  1.7280   2.985984   43.3039   22.785137         0.0        0.0   \n",
       "836    0.0  6.1026  37.241727  106.6334   42.920000         0.0        0.0   \n",
       "\n",
       "     nAtom  nHeavyAtom    nH  ...        AMW      WTPT-1    WTPT-2     WTPT-3  \\\n",
       "0     35.0        11.0  24.0  ...   4.462509   20.828762  1.893524   0.000000   \n",
       "1     13.0         5.0   8.0  ...   5.542886    9.148995  1.829799   2.705816   \n",
       "2     30.0        12.0  18.0  ...   5.937350   22.828595  1.902383  11.163248   \n",
       "3      5.0         3.0   2.0  ...   9.201096    4.914214  1.638071   4.414214   \n",
       "4     34.0        14.0  20.0  ...   6.004005   26.661622  1.904402  11.202922   \n",
       "5     28.0        12.0  16.0  ...   6.289459   22.661828  1.888486  11.077237   \n",
       "6      9.0         5.0   4.0  ...  10.665346    8.500000  1.700000   9.750000   \n",
       "7     38.0        14.0  24.0  ...   5.267832   26.491096  1.892221   5.447103   \n",
       "8     18.0         8.0  10.0  ...   5.893236   15.532958  1.941620   0.000000   \n",
       "9     17.0        12.0   5.0  ...   9.824815   23.243258  1.936938  12.697752   \n",
       "10    17.0        11.0   6.0  ...   8.590399   22.509724  2.046339   5.588995   \n",
       "11    17.0         5.0  12.0  ...   4.240818    8.849874  1.769975   0.000000   \n",
       "12    20.0         6.0  14.0  ...   4.305478   10.679177  1.779863   0.000000   \n",
       "13    20.0         6.0  14.0  ...   4.305478   10.839150  1.806525   0.000000   \n",
       "14    29.0         9.0  20.0  ...   4.419190   16.829768  1.869974   0.000000   \n",
       "15    38.0        13.0  25.0  ...   5.372749   24.828511  1.909885   2.414011   \n",
       "16    50.0        17.0  33.0  ...   6.083531   32.828432  1.931084   2.414201   \n",
       "17    19.0         6.0  13.0  ...   4.584463   10.839150  1.806525   2.388325   \n",
       "18    13.0         4.0   9.0  ...   4.544115    6.732051  1.683013   2.732051   \n",
       "19     6.0         2.0   4.0  ...   5.337702    3.000000  1.500000   2.000000   \n",
       "20    18.0         6.0  12.0  ...   4.893823   10.839150  1.806525   2.388325   \n",
       "21    10.0         4.0   6.0  ...   5.804186    7.175869  1.793967   2.632592   \n",
       "22    10.0         4.0   6.0  ...   6.203678    6.871320  1.717830   4.621320   \n",
       "23    13.0         5.0   8.0  ...   5.850187    8.696802  1.739360   4.642168   \n",
       "24    16.0         6.0  10.0  ...   5.629255   10.675851  1.779308   4.745721   \n",
       "25     8.0         4.0   4.0  ...   7.003277    6.871320  1.717830   2.310660   \n",
       "26    16.0         6.0  10.0  ...   5.629255   10.679177  1.779863   4.994504   \n",
       "27    22.0         8.0  14.0  ...   5.368154   14.831108  1.853888   5.297907   \n",
       "28    10.0         4.0   6.0  ...   5.804186    6.871320  1.717830   2.310660   \n",
       "29    27.0        11.0  16.0  ...   5.634078   20.486117  1.862374   2.404409   \n",
       "..     ...         ...   ...  ...        ...         ...       ...        ...   \n",
       "807   25.0        13.0  12.0  ...   7.203595   25.261329  1.943179  10.649353   \n",
       "808   23.0        19.0   4.0  ...  27.718967   37.688465  1.983603  18.233900   \n",
       "809   20.0        11.0   9.0  ...   7.553166   21.405281  1.945935   7.599746   \n",
       "810   30.0        15.0  15.0  ...   6.970173   29.259042  1.950603  11.139799   \n",
       "811   33.0        20.0  13.0  ...  10.484094   39.888535  1.994427  17.231094   \n",
       "812   34.0        16.0  18.0  ...   6.180613   32.361629  2.022602   0.000000   \n",
       "813   40.0        18.0  22.0  ...   5.954304   37.385806  2.076989   0.000000   \n",
       "814   29.0        17.0  12.0  ...   7.864781   34.131858  2.007756   7.903389   \n",
       "815   24.0        15.0   9.0  ...   9.292930   29.988912  1.999261  12.983253   \n",
       "816   25.0        16.0   9.0  ...   9.561009   31.835518  1.989720  15.521978   \n",
       "817   25.0        16.0   9.0  ...   9.561009   31.835601  1.989725  15.520819   \n",
       "818   28.0        18.0  10.0  ...   8.787601   35.812338  1.989574  12.635009   \n",
       "819   15.0         7.0   8.0  ...   8.001633   13.370400  1.910057   8.151650   \n",
       "820   33.0        17.0  16.0  ...   8.002288   35.365803  2.080341  12.501579   \n",
       "821   12.0         8.0   4.0  ...   9.335606   15.532829  1.941604  10.860943   \n",
       "822   57.0        27.0  30.0  ...   6.846012   53.066049  1.965409  36.266241   \n",
       "823   19.0         9.0  10.0  ...   8.527818   17.382017  1.931335  11.754136   \n",
       "824   33.0        20.0  13.0  ...   7.972565   41.097444  2.054872   8.201207   \n",
       "825   86.0        54.0  32.0  ...   9.489576  108.041662  2.000772  49.803260   \n",
       "826    9.0         5.0   4.0  ...   7.558468    9.687500  1.937500   2.875000   \n",
       "827   24.0        15.0   9.0  ...   9.292930   30.118295  2.007886  14.539285   \n",
       "828   22.0        13.0   9.0  ...   9.818227   26.294966  2.022690  14.916781   \n",
       "829   47.0        27.0  20.0  ...   8.002942   54.270431  2.010016  27.826057   \n",
       "830   32.0        17.0  15.0  ...   7.534219   33.243252  1.955485  15.993278   \n",
       "831   11.0         5.0   6.0  ...  14.175831    8.849874  1.769975   4.724874   \n",
       "832    8.0         3.0   5.0  ...  13.494683    4.914214  1.638071   2.207107   \n",
       "833   80.0        31.0  49.0  ...   5.591950   59.942890  1.933642  17.740711   \n",
       "834   20.0        10.0  10.0  ...   7.103711   19.169390  1.916939  10.899136   \n",
       "835   20.0        11.0   9.0  ...   7.553166   21.406303  1.946028   7.858999   \n",
       "836   22.0        22.0   0.0  ...  22.440388   43.029507  1.955887  25.164391   \n",
       "\n",
       "        WTPT-4     WTPT-5    WPATH  WPOL  XLogP  Zagreb  \n",
       "0     0.000000   0.000000    220.0   8.0  6.599    38.0  \n",
       "1     2.705816   0.000000     17.0   2.0  0.628    22.0  \n",
       "2    11.163248   0.000000    286.0   9.0 -0.646    42.0  \n",
       "3     4.414214   0.000000      4.0   0.0 -0.316     6.0  \n",
       "4    11.202922   0.000000    444.0  11.0  1.211    52.0  \n",
       "5    11.077237   0.000000    277.0   9.0  0.284    44.0  \n",
       "6     6.750000   0.000000     16.0   0.0 -0.837    20.0  \n",
       "7     5.447103   0.000000    372.0  14.0  4.417    54.0  \n",
       "8     0.000000   0.000000     61.0   7.0  2.866    36.0  \n",
       "9     9.666664   3.031088    188.0  16.0  0.777    56.0  \n",
       "10    5.588995   0.000000    144.0  14.0  1.022    56.0  \n",
       "11    0.000000   0.000000     20.0   2.0  3.185    14.0  \n",
       "12    0.000000   0.000000     32.0   3.0  3.693    20.0  \n",
       "13    0.000000   0.000000     35.0   3.0  3.754    18.0  \n",
       "14    0.000000   0.000000    120.0   6.0  5.461    30.0  \n",
       "15    0.000000   0.000000    364.0  10.0  6.877    46.0  \n",
       "16    0.000000   0.000000    816.0  14.0  9.491    62.0  \n",
       "17    0.000000   2.388325     35.0   3.0  1.353    18.0  \n",
       "18    0.000000   2.732051      9.0   0.0  0.063    12.0  \n",
       "19    2.000000   0.000000      1.0   0.0 -0.499     2.0  \n",
       "20    2.388325   0.000000     35.0   3.0  1.420    18.0  \n",
       "21    2.632592   0.000000      8.0   0.0  0.270    18.0  \n",
       "22    4.621320   0.000000     10.0   1.0 -1.208    10.0  \n",
       "23    4.642168   0.000000     18.0   2.0 -0.748    16.0  \n",
       "24    4.745721   0.000000     31.0   4.0 -0.390    20.0  \n",
       "25    2.310660   0.000000     10.0   1.0 -0.231    10.0  \n",
       "26    4.994504   0.000000     32.0   3.0 -0.229    20.0  \n",
       "27    5.297907   0.000000     84.0   5.0  0.661    26.0  \n",
       "28    2.310660   0.000000     10.0   1.0  0.587    10.0  \n",
       "29    2.404409   0.000000    194.0   9.0  2.701    42.0  \n",
       "..         ...        ...      ...   ...    ...     ...  \n",
       "807   5.169871   5.479482    260.0  16.0  0.143    60.0  \n",
       "808   3.108668   0.000000    696.0  31.0  6.259    98.0  \n",
       "809   4.825813   2.773933    162.0  13.0  0.320    50.0  \n",
       "810   8.441999   2.697800    384.0  18.0  1.689    68.0  \n",
       "811   5.984388   0.000000    798.0  30.0  4.763   100.0  \n",
       "812   0.000000   0.000000    416.0  24.0  5.630    78.0  \n",
       "813   0.000000   0.000000    657.0  25.0  6.614    92.0  \n",
       "814   7.903389   0.000000    528.0  25.0  2.221    84.0  \n",
       "815   7.158431   2.552534    322.0  24.0  1.091    80.0  \n",
       "816   9.715200   2.540473    406.0  25.0 -0.910    86.0  \n",
       "817   9.714803   2.539611    409.0  25.0 -0.910    86.0  \n",
       "818  12.635009   0.000000    604.0  29.0  1.963    92.0  \n",
       "819   4.825825   0.000000     39.0   4.0  0.466    34.0  \n",
       "820   0.000000   6.243204    573.0  21.0  2.536    88.0  \n",
       "821   4.990326   5.870617     62.0   7.0  0.190    36.0  \n",
       "822  16.492335  19.773906   1818.0  39.0  0.003   120.0  \n",
       "823   0.000000   6.326638     84.0  10.0  0.779    42.0  \n",
       "824   5.143169   3.058038    833.0  30.0  2.736   104.0  \n",
       "825  21.122418  18.451260  15287.0  92.0  7.164   274.0  \n",
       "826   2.875000   0.000000     15.0   0.0  0.709    20.0  \n",
       "827   8.001878   3.051381    351.0  22.0  0.850    78.0  \n",
       "828   8.851783   0.000000    230.0  19.0  2.151    68.0  \n",
       "829  14.963556  12.862501   1698.0  49.0 -2.049   144.0  \n",
       "830  13.613177   2.380101    606.0  20.0 -0.223    76.0  \n",
       "831   0.000000   0.000000     20.0   2.0  2.436    14.0  \n",
       "832   0.000000   0.000000      4.0   0.0  1.736     6.0  \n",
       "833  14.775250   2.965461   4310.0  37.0  5.367   128.0  \n",
       "834   5.002370   5.896766    113.0  11.0  0.292    48.0  \n",
       "835   4.830427   3.028572    150.0  14.0  1.889    50.0  \n",
       "836   0.000000   0.000000    907.0  46.0  7.156   118.0  \n",
       "\n",
       "[837 rows x 1423 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = biodeg[biodeg['Status'].str.contains('Train')]\n",
    "train_y = encode(train_x['Class'])\n",
    "\n",
    "# manipulate data type of x data to float\n",
    "train_x = train_x.iloc[:,4:1449]\n",
    "train_x.drop(train_x.select_dtypes(['object']), inplace=True, axis=1)\n",
    "train_x.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nAcid</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>AMR</th>\n",
       "      <th>apol</th>\n",
       "      <th>naAromAtom</th>\n",
       "      <th>nAromBond</th>\n",
       "      <th>nAtom</th>\n",
       "      <th>nHeavyAtom</th>\n",
       "      <th>nH</th>\n",
       "      <th>...</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WTPT-1</th>\n",
       "      <th>WTPT-2</th>\n",
       "      <th>WTPT-3</th>\n",
       "      <th>WTPT-4</th>\n",
       "      <th>WTPT-5</th>\n",
       "      <th>WPATH</th>\n",
       "      <th>WPOL</th>\n",
       "      <th>XLogP</th>\n",
       "      <th>Zagreb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.3374</td>\n",
       "      <td>1.788639</td>\n",
       "      <td>26.3554</td>\n",
       "      <td>22.988688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.353270</td>\n",
       "      <td>12.833789</td>\n",
       "      <td>1.833398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.323</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1309</td>\n",
       "      <td>0.017135</td>\n",
       "      <td>11.0435</td>\n",
       "      <td>6.989172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.289459</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8498</td>\n",
       "      <td>3.421760</td>\n",
       "      <td>35.2378</td>\n",
       "      <td>17.654344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.137507</td>\n",
       "      <td>13.676825</td>\n",
       "      <td>1.953832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.474</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.4466</td>\n",
       "      <td>2.092652</td>\n",
       "      <td>36.1682</td>\n",
       "      <td>27.849067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.612562</td>\n",
       "      <td>16.829768</td>\n",
       "      <td>1.869974</td>\n",
       "      <td>2.926777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.926777</td>\n",
       "      <td>120.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.524</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4692</td>\n",
       "      <td>6.096949</td>\n",
       "      <td>49.7536</td>\n",
       "      <td>30.404274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.315026</td>\n",
       "      <td>20.254935</td>\n",
       "      <td>1.841358</td>\n",
       "      <td>2.400011</td>\n",
       "      <td>2.400011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.468</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.2352</td>\n",
       "      <td>1.525719</td>\n",
       "      <td>41.0857</td>\n",
       "      <td>28.488274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.590538</td>\n",
       "      <td>20.828762</td>\n",
       "      <td>1.893524</td>\n",
       "      <td>8.289840</td>\n",
       "      <td>8.289840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.471</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3943</td>\n",
       "      <td>0.155472</td>\n",
       "      <td>19.6534</td>\n",
       "      <td>13.176344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.542886</td>\n",
       "      <td>8.849874</td>\n",
       "      <td>1.769975</td>\n",
       "      <td>2.362437</td>\n",
       "      <td>2.362437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.156</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0853</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>31.6522</td>\n",
       "      <td>20.165516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.804186</td>\n",
       "      <td>14.279081</td>\n",
       "      <td>1.784885</td>\n",
       "      <td>4.686998</td>\n",
       "      <td>4.686998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.183</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1960</td>\n",
       "      <td>0.038416</td>\n",
       "      <td>35.9868</td>\n",
       "      <td>23.529516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.959720</td>\n",
       "      <td>20.482651</td>\n",
       "      <td>1.862059</td>\n",
       "      <td>10.256638</td>\n",
       "      <td>10.256638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>192.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.290</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.5284</td>\n",
       "      <td>2.336007</td>\n",
       "      <td>87.7498</td>\n",
       "      <td>62.412548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.671023</td>\n",
       "      <td>46.147682</td>\n",
       "      <td>1.922820</td>\n",
       "      <td>11.003383</td>\n",
       "      <td>11.003383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1794.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.934</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.6964</td>\n",
       "      <td>22.056173</td>\n",
       "      <td>64.3921</td>\n",
       "      <td>58.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.267832</td>\n",
       "      <td>40.481794</td>\n",
       "      <td>1.927704</td>\n",
       "      <td>7.279942</td>\n",
       "      <td>7.279942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.623</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.4872</td>\n",
       "      <td>20.134964</td>\n",
       "      <td>63.2321</td>\n",
       "      <td>59.211341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.899870</td>\n",
       "      <td>38.828428</td>\n",
       "      <td>1.941421</td>\n",
       "      <td>2.414212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.414212</td>\n",
       "      <td>1330.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.388</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4074</td>\n",
       "      <td>5.795575</td>\n",
       "      <td>45.1176</td>\n",
       "      <td>20.680758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.665644</td>\n",
       "      <td>17.528614</td>\n",
       "      <td>1.947624</td>\n",
       "      <td>4.815165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.281</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3417</td>\n",
       "      <td>0.116759</td>\n",
       "      <td>39.2048</td>\n",
       "      <td>19.258344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.297202</td>\n",
       "      <td>17.548927</td>\n",
       "      <td>1.949881</td>\n",
       "      <td>5.273677</td>\n",
       "      <td>5.273677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.975</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7591</td>\n",
       "      <td>0.576233</td>\n",
       "      <td>37.0125</td>\n",
       "      <td>18.456344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.753595</td>\n",
       "      <td>15.692665</td>\n",
       "      <td>1.961583</td>\n",
       "      <td>2.452455</td>\n",
       "      <td>2.452455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.593</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>49.3166</td>\n",
       "      <td>24.913930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.548318</td>\n",
       "      <td>23.440382</td>\n",
       "      <td>1.953365</td>\n",
       "      <td>8.039140</td>\n",
       "      <td>8.039140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>198.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.910</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4144</td>\n",
       "      <td>2.000527</td>\n",
       "      <td>41.2370</td>\n",
       "      <td>21.018344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.558468</td>\n",
       "      <td>19.385708</td>\n",
       "      <td>1.938571</td>\n",
       "      <td>4.818143</td>\n",
       "      <td>4.818143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.374</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4144</td>\n",
       "      <td>2.000527</td>\n",
       "      <td>41.2370</td>\n",
       "      <td>21.018344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.558468</td>\n",
       "      <td>19.385511</td>\n",
       "      <td>1.938551</td>\n",
       "      <td>4.817250</td>\n",
       "      <td>4.817250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.374</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0442</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>65.0770</td>\n",
       "      <td>35.267102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.939970</td>\n",
       "      <td>35.242254</td>\n",
       "      <td>1.957903</td>\n",
       "      <td>15.757946</td>\n",
       "      <td>15.757946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>721.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6794</td>\n",
       "      <td>0.461584</td>\n",
       "      <td>37.5398</td>\n",
       "      <td>17.555965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.502651</td>\n",
       "      <td>17.549594</td>\n",
       "      <td>1.949955</td>\n",
       "      <td>4.961358</td>\n",
       "      <td>2.506669</td>\n",
       "      <td>2.454688</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.376</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8615</td>\n",
       "      <td>0.742182</td>\n",
       "      <td>45.4849</td>\n",
       "      <td>22.960344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.053653</td>\n",
       "      <td>21.156620</td>\n",
       "      <td>1.923329</td>\n",
       "      <td>10.368954</td>\n",
       "      <td>7.121372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>152.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5168</td>\n",
       "      <td>0.267082</td>\n",
       "      <td>42.6731</td>\n",
       "      <td>21.018344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.558468</td>\n",
       "      <td>19.568743</td>\n",
       "      <td>1.956874</td>\n",
       "      <td>5.232924</td>\n",
       "      <td>5.232924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>125.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.670</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9353</td>\n",
       "      <td>0.874786</td>\n",
       "      <td>56.3943</td>\n",
       "      <td>39.955446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.951497</td>\n",
       "      <td>31.209663</td>\n",
       "      <td>1.950604</td>\n",
       "      <td>7.712862</td>\n",
       "      <td>7.712862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>482.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.655</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.6748</td>\n",
       "      <td>2.804955</td>\n",
       "      <td>18.8845</td>\n",
       "      <td>10.147172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.559716</td>\n",
       "      <td>9.687500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.7378</td>\n",
       "      <td>0.544349</td>\n",
       "      <td>29.0669</td>\n",
       "      <td>14.035965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.919759</td>\n",
       "      <td>13.676825</td>\n",
       "      <td>1.953832</td>\n",
       "      <td>5.434832</td>\n",
       "      <td>2.490697</td>\n",
       "      <td>2.944136</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4384</td>\n",
       "      <td>0.192195</td>\n",
       "      <td>26.3209</td>\n",
       "      <td>18.561516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.671883</td>\n",
       "      <td>10.839150</td>\n",
       "      <td>1.806525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.388</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9410</td>\n",
       "      <td>0.885481</td>\n",
       "      <td>23.7944</td>\n",
       "      <td>12.997551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.088465</td>\n",
       "      <td>6.732051</td>\n",
       "      <td>1.683013</td>\n",
       "      <td>2.244017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.033</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.2630</td>\n",
       "      <td>1.595169</td>\n",
       "      <td>20.6562</td>\n",
       "      <td>15.474723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.568072</td>\n",
       "      <td>8.849874</td>\n",
       "      <td>1.769975</td>\n",
       "      <td>2.362437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.362437</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.784</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.4294</td>\n",
       "      <td>19.619584</td>\n",
       "      <td>53.9379</td>\n",
       "      <td>51.632962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.750215</td>\n",
       "      <td>32.828432</td>\n",
       "      <td>1.931084</td>\n",
       "      <td>2.414201</td>\n",
       "      <td>2.414201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>816.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.679</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4889</td>\n",
       "      <td>0.239023</td>\n",
       "      <td>19.3048</td>\n",
       "      <td>12.218344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.850187</td>\n",
       "      <td>8.849874</td>\n",
       "      <td>1.769975</td>\n",
       "      <td>4.996320</td>\n",
       "      <td>4.996320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2452</td>\n",
       "      <td>10.531323</td>\n",
       "      <td>42.0302</td>\n",
       "      <td>17.053586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.711125</td>\n",
       "      <td>10.523646</td>\n",
       "      <td>1.753941</td>\n",
       "      <td>9.301424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.928</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2977</td>\n",
       "      <td>5.279425</td>\n",
       "      <td>21.8165</td>\n",
       "      <td>10.217793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.992593</td>\n",
       "      <td>12.293055</td>\n",
       "      <td>1.756151</td>\n",
       "      <td>11.587713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.818</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1573</td>\n",
       "      <td>4.653943</td>\n",
       "      <td>29.4885</td>\n",
       "      <td>15.400758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.332055</td>\n",
       "      <td>10.675851</td>\n",
       "      <td>1.779308</td>\n",
       "      <td>4.745721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.025</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.2427</td>\n",
       "      <td>85.427503</td>\n",
       "      <td>66.0417</td>\n",
       "      <td>37.259000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.773999</td>\n",
       "      <td>73.753328</td>\n",
       "      <td>1.843833</td>\n",
       "      <td>67.907675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.307146</td>\n",
       "      <td>4131.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>11.748</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3809</td>\n",
       "      <td>1.906885</td>\n",
       "      <td>31.6991</td>\n",
       "      <td>17.536344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.466348</td>\n",
       "      <td>12.833789</td>\n",
       "      <td>1.833398</td>\n",
       "      <td>7.656092</td>\n",
       "      <td>2.853553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.550</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5390</td>\n",
       "      <td>0.290521</td>\n",
       "      <td>28.8034</td>\n",
       "      <td>18.405516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.478091</td>\n",
       "      <td>12.670365</td>\n",
       "      <td>1.810052</td>\n",
       "      <td>5.199368</td>\n",
       "      <td>5.199368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.194</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8532</td>\n",
       "      <td>3.434350</td>\n",
       "      <td>43.2598</td>\n",
       "      <td>28.644274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.076277</td>\n",
       "      <td>18.260598</td>\n",
       "      <td>1.826060</td>\n",
       "      <td>2.399340</td>\n",
       "      <td>2.399340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>131.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.916</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4958</td>\n",
       "      <td>2.237418</td>\n",
       "      <td>27.2329</td>\n",
       "      <td>11.528793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.844185</td>\n",
       "      <td>10.465990</td>\n",
       "      <td>1.744332</td>\n",
       "      <td>9.237437</td>\n",
       "      <td>2.332107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.444</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0207</td>\n",
       "      <td>36.248828</td>\n",
       "      <td>73.2098</td>\n",
       "      <td>70.627685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.918118</td>\n",
       "      <td>46.661553</td>\n",
       "      <td>1.944231</td>\n",
       "      <td>4.764176</td>\n",
       "      <td>2.382088</td>\n",
       "      <td>2.382088</td>\n",
       "      <td>2279.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.256</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2461</td>\n",
       "      <td>0.060565</td>\n",
       "      <td>15.5367</td>\n",
       "      <td>14.223551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.462560</td>\n",
       "      <td>10.675851</td>\n",
       "      <td>1.779308</td>\n",
       "      <td>10.624436</td>\n",
       "      <td>7.653238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.5020</td>\n",
       "      <td>6.260004</td>\n",
       "      <td>69.0458</td>\n",
       "      <td>55.242169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.815314</td>\n",
       "      <td>38.423081</td>\n",
       "      <td>1.921154</td>\n",
       "      <td>15.175200</td>\n",
       "      <td>11.573295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.067</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.8079</td>\n",
       "      <td>14.500102</td>\n",
       "      <td>41.6619</td>\n",
       "      <td>24.705793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.922065</td>\n",
       "      <td>45.785614</td>\n",
       "      <td>1.831425</td>\n",
       "      <td>40.595154</td>\n",
       "      <td>4.710243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.905</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7426</td>\n",
       "      <td>7.521855</td>\n",
       "      <td>43.8018</td>\n",
       "      <td>23.841516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.718757</td>\n",
       "      <td>17.382017</td>\n",
       "      <td>1.931335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.258</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.1133</td>\n",
       "      <td>16.919237</td>\n",
       "      <td>72.9088</td>\n",
       "      <td>29.570379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.107677</td>\n",
       "      <td>22.897487</td>\n",
       "      <td>1.908124</td>\n",
       "      <td>12.421431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>174.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.762</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9900</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>38.0554</td>\n",
       "      <td>18.094344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.754297</td>\n",
       "      <td>15.532829</td>\n",
       "      <td>1.941604</td>\n",
       "      <td>4.990326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.990326</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.8650</td>\n",
       "      <td>0.748225</td>\n",
       "      <td>46.6992</td>\n",
       "      <td>23.749930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.146634</td>\n",
       "      <td>21.392597</td>\n",
       "      <td>1.944782</td>\n",
       "      <td>7.845920</td>\n",
       "      <td>2.395580</td>\n",
       "      <td>5.450340</td>\n",
       "      <td>166.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6473</td>\n",
       "      <td>7.008197</td>\n",
       "      <td>54.7500</td>\n",
       "      <td>30.830688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.861433</td>\n",
       "      <td>23.005284</td>\n",
       "      <td>1.917107</td>\n",
       "      <td>2.513691</td>\n",
       "      <td>2.513691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>181.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.688</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1795</td>\n",
       "      <td>4.750220</td>\n",
       "      <td>45.4073</td>\n",
       "      <td>24.643516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.185855</td>\n",
       "      <td>19.224956</td>\n",
       "      <td>1.922496</td>\n",
       "      <td>2.485848</td>\n",
       "      <td>2.485848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.700</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0937</td>\n",
       "      <td>9.570980</td>\n",
       "      <td>59.0320</td>\n",
       "      <td>33.924274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.746315</td>\n",
       "      <td>24.847459</td>\n",
       "      <td>1.911343</td>\n",
       "      <td>2.514398</td>\n",
       "      <td>2.514398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>224.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.080</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6554</td>\n",
       "      <td>2.740349</td>\n",
       "      <td>41.5595</td>\n",
       "      <td>19.969551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.876159</td>\n",
       "      <td>17.382017</td>\n",
       "      <td>1.931335</td>\n",
       "      <td>4.981112</td>\n",
       "      <td>2.489663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.704</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0864</td>\n",
       "      <td>1.180265</td>\n",
       "      <td>45.3792</td>\n",
       "      <td>21.356344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.237034</td>\n",
       "      <td>17.549594</td>\n",
       "      <td>1.949955</td>\n",
       "      <td>5.271016</td>\n",
       "      <td>2.506669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.432</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9920</td>\n",
       "      <td>8.952064</td>\n",
       "      <td>68.0450</td>\n",
       "      <td>38.777860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.832898</td>\n",
       "      <td>29.017579</td>\n",
       "      <td>1.934505</td>\n",
       "      <td>2.406054</td>\n",
       "      <td>2.406054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>402.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.112</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7133</td>\n",
       "      <td>2.935397</td>\n",
       "      <td>43.2787</td>\n",
       "      <td>22.530758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.901384</td>\n",
       "      <td>26.933327</td>\n",
       "      <td>1.923809</td>\n",
       "      <td>18.226967</td>\n",
       "      <td>12.176220</td>\n",
       "      <td>6.050747</td>\n",
       "      <td>286.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.208</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6427</td>\n",
       "      <td>6.983863</td>\n",
       "      <td>81.5180</td>\n",
       "      <td>42.257895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.766297</td>\n",
       "      <td>37.142943</td>\n",
       "      <td>1.954892</td>\n",
       "      <td>20.311353</td>\n",
       "      <td>8.752453</td>\n",
       "      <td>5.520549</td>\n",
       "      <td>760.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.629</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5790</td>\n",
       "      <td>2.493241</td>\n",
       "      <td>70.6894</td>\n",
       "      <td>35.849516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.135785</td>\n",
       "      <td>36.092058</td>\n",
       "      <td>2.005114</td>\n",
       "      <td>10.416198</td>\n",
       "      <td>10.416198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>633.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.666</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5736</td>\n",
       "      <td>0.329017</td>\n",
       "      <td>45.2804</td>\n",
       "      <td>29.077481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.540393</td>\n",
       "      <td>20.928005</td>\n",
       "      <td>1.902546</td>\n",
       "      <td>5.386377</td>\n",
       "      <td>2.490046</td>\n",
       "      <td>2.896331</td>\n",
       "      <td>138.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.003</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2495</td>\n",
       "      <td>1.561250</td>\n",
       "      <td>99.6924</td>\n",
       "      <td>56.200653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.296623</td>\n",
       "      <td>58.481230</td>\n",
       "      <td>1.949374</td>\n",
       "      <td>34.271881</td>\n",
       "      <td>23.959978</td>\n",
       "      <td>10.311903</td>\n",
       "      <td>2679.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9139</td>\n",
       "      <td>8.490813</td>\n",
       "      <td>126.9222</td>\n",
       "      <td>64.950653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.206655</td>\n",
       "      <td>66.773197</td>\n",
       "      <td>2.023430</td>\n",
       "      <td>25.562011</td>\n",
       "      <td>10.402495</td>\n",
       "      <td>15.159515</td>\n",
       "      <td>4050.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>3.873</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.265478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4689.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1530</td>\n",
       "      <td>9.941409</td>\n",
       "      <td>68.9038</td>\n",
       "      <td>30.814344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.090923</td>\n",
       "      <td>28.306077</td>\n",
       "      <td>2.021863</td>\n",
       "      <td>5.093771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>287.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.962</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218 rows Ã— 1423 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nAcid   ALogP     ALogp2       AMR       apol  naAromAtom  nAromBond  \\\n",
       "837     0.0 -1.3374   1.788639   26.3554  22.988688         0.0        0.0   \n",
       "838     0.0 -0.1309   0.017135   11.0435   6.989172         0.0        0.0   \n",
       "839     0.0  1.8498   3.421760   35.2378  17.654344         0.0        0.0   \n",
       "840     0.0 -1.4466   2.092652   36.1682  27.849067         0.0        0.0   \n",
       "841     0.0  2.4692   6.096949   49.7536  30.404274         0.0        0.0   \n",
       "842     0.0 -1.2352   1.525719   41.0857  28.488274         0.0        0.0   \n",
       "843     0.0 -0.3943   0.155472   19.6534  13.176344         0.0        0.0   \n",
       "844     0.0 -0.0853   0.007276   31.6522  20.165516         0.0        0.0   \n",
       "845     0.0 -0.1960   0.038416   35.9868  23.529516         0.0        0.0   \n",
       "846     0.0 -1.5284   2.336007   87.7498  62.412548         0.0        0.0   \n",
       "847     1.0 -4.6964  22.056173   64.3921  58.090548         0.0        0.0   \n",
       "848     0.0 -4.4872  20.134964   63.2321  59.211341         0.0        0.0   \n",
       "849     0.0  2.4074   5.795575   45.1176  20.680758         0.0        0.0   \n",
       "850     0.0  0.3417   0.116759   39.2048  19.258344         0.0        0.0   \n",
       "851     0.0  0.7591   0.576233   37.0125  18.456344         0.0        0.0   \n",
       "852     0.0  0.0182   0.000331   49.3166  24.913930         0.0        0.0   \n",
       "853     1.0  1.4144   2.000527   41.2370  21.018344         0.0        0.0   \n",
       "854     1.0  1.4144   2.000527   41.2370  21.018344         0.0        0.0   \n",
       "855     0.0 -0.0442   0.001954   65.0770  35.267102         0.0        0.0   \n",
       "856     0.0  0.6794   0.461584   37.5398  17.555965         0.0        0.0   \n",
       "857     1.0  0.8615   0.742182   45.4849  22.960344         0.0        0.0   \n",
       "858     0.0  0.5168   0.267082   42.6731  21.018344         0.0        0.0   \n",
       "859     0.0 -0.9353   0.874786   56.3943  39.955446         0.0        0.0   \n",
       "860     0.0 -1.6748   2.804955   18.8845  10.147172         0.0        0.0   \n",
       "861     0.0 -0.7378   0.544349   29.0669  14.035965         0.0        0.0   \n",
       "862     0.0  0.4384   0.192195   26.3209  18.561516         0.0        0.0   \n",
       "863     0.0  0.9410   0.885481   23.7944  12.997551         0.0        0.0   \n",
       "864     0.0 -1.2630   1.595169   20.6562  15.474723         0.0        0.0   \n",
       "865     0.0 -4.4294  19.619584   53.9379  51.632962         0.0        0.0   \n",
       "866     0.0 -0.4889   0.239023   19.3048  12.218344         0.0        0.0   \n",
       "...     ...     ...        ...       ...        ...         ...        ...   \n",
       "1025    0.0  3.2452  10.531323   42.0302  17.053586         0.0        0.0   \n",
       "1026    0.0  2.2977   5.279425   21.8165  10.217793         0.0        0.0   \n",
       "1027    0.0  2.1573   4.653943   29.4885  15.400758         0.0        0.0   \n",
       "1028    0.0  9.2427  85.427503   66.0417  37.259000         0.0        0.0   \n",
       "1029    0.0  1.3809   1.906885   31.6991  17.536344         0.0        0.0   \n",
       "1030    0.0 -0.5390   0.290521   28.8034  18.405516         0.0        0.0   \n",
       "1031    0.0  1.8532   3.434350   43.2598  28.644274         0.0        0.0   \n",
       "1032    0.0  1.4958   2.237418   27.2329  11.528793         0.0        0.0   \n",
       "1033    0.0 -6.0207  36.248828   73.2098  70.627685         0.0        0.0   \n",
       "1034    0.0  0.2461   0.060565   15.5367  14.223551         0.0        0.0   \n",
       "1035    0.0 -2.5020   6.260004   69.0458  55.242169         0.0        0.0   \n",
       "1036    1.0  3.8079  14.500102   41.6619  24.705793         0.0        0.0   \n",
       "1037    0.0  2.7426   7.521855   43.8018  23.841516         0.0        0.0   \n",
       "1038    0.0  4.1133  16.919237   72.9088  29.570379         0.0        0.0   \n",
       "1039    0.0 -0.9900   0.980100   38.0554  18.094344         0.0        0.0   \n",
       "1040    0.0 -0.8650   0.748225   46.6992  23.749930         0.0        0.0   \n",
       "1041    0.0  2.6473   7.008197   54.7500  30.830688         0.0        0.0   \n",
       "1042    0.0  2.1795   4.750220   45.4073  24.643516         0.0        0.0   \n",
       "1043    0.0  3.0937   9.570980   59.0320  33.924274         0.0        0.0   \n",
       "1044    0.0  1.6554   2.740349   41.5595  19.969551         0.0        0.0   \n",
       "1045    0.0  1.0864   1.180265   45.3792  21.356344         0.0        0.0   \n",
       "1046    0.0  2.9920   8.952064   68.0450  38.777860         0.0        0.0   \n",
       "1047    0.0  1.7133   2.935397   43.2787  22.530758         0.0        0.0   \n",
       "1048    0.0  2.6427   6.983863   81.5180  42.257895         0.0        0.0   \n",
       "1049    0.0  1.5790   2.493241   70.6894  35.849516         0.0        0.0   \n",
       "1050    0.0  0.5736   0.329017   45.2804  29.077481         0.0        0.0   \n",
       "1051    0.0  1.2495   1.561250   99.6924  56.200653         0.0        0.0   \n",
       "1052    0.0  2.9139   8.490813  126.9222  64.950653         0.0        0.0   \n",
       "1053    0.0  0.0000   0.000000    0.0000   0.000000         0.0        0.0   \n",
       "1054    0.0  3.1530   9.941409   68.9038  30.814344         0.0        0.0   \n",
       "\n",
       "      nAtom  nHeavyAtom    nH  ...        AMW     WTPT-1    WTPT-2     WTPT-3  \\\n",
       "837    23.0         7.0  16.0  ...   4.353270  12.833789  1.833398   0.000000   \n",
       "838     7.0         3.0   4.0  ...   6.289459   5.250000  1.750000   2.500000   \n",
       "839    15.0         7.0   8.0  ...   6.137507  13.676825  1.953832   0.000000   \n",
       "840    28.0         9.0  19.0  ...   4.612562  16.829768  1.869974   2.926777   \n",
       "841    29.0        11.0  18.0  ...   5.315026  20.254935  1.841358   2.400011   \n",
       "842    29.0        11.0  18.0  ...   5.590538  20.828762  1.893524   8.289840   \n",
       "843    13.0         5.0   8.0  ...   5.542886   8.849874  1.769975   2.362437   \n",
       "844    20.0         8.0  12.0  ...   5.804186  14.279081  1.784885   4.686998   \n",
       "845    23.0        11.0  12.0  ...   6.959720  20.482651  1.862059  10.256638   \n",
       "846    60.0        24.0  36.0  ...   5.671023  46.147682  1.922820  11.003383   \n",
       "847    57.0        21.0  36.0  ...   5.267832  40.481794  1.927704   7.279942   \n",
       "848    57.0        20.0  37.0  ...   4.899870  38.828428  1.941421   2.414212   \n",
       "849    15.0         9.0   6.0  ...  10.665644  17.528614  1.947624   4.815165   \n",
       "850    17.0         9.0   8.0  ...   7.297202  17.548927  1.949881   5.273677   \n",
       "851    16.0         8.0   8.0  ...   6.753595  15.692665  1.961583   2.452455   \n",
       "852    22.0        12.0  10.0  ...   7.548318  23.440382  1.953365   8.039140   \n",
       "853    18.0        10.0   8.0  ...   7.558468  19.385708  1.938571   4.818143   \n",
       "854    18.0        10.0   8.0  ...   7.558468  19.385511  1.938551   4.817250   \n",
       "855    32.0        18.0  14.0  ...   7.939970  35.242254  1.957903  15.757946   \n",
       "856    14.0         9.0   5.0  ...   8.502651  17.549594  1.949955   4.961358   \n",
       "857    19.0        11.0   8.0  ...   9.053653  21.156620  1.923329  10.368954   \n",
       "858    18.0        10.0   8.0  ...   7.558468  19.568743  1.956874   5.232924   \n",
       "859    38.0        16.0  22.0  ...   5.951497  31.209663  1.950604   7.712862   \n",
       "860     9.0         5.0   4.0  ...   7.559716   9.687500  1.937500   5.750000   \n",
       "861    12.0         7.0   5.0  ...   7.919759  13.676825  1.953832   5.434832   \n",
       "862    18.0         6.0  12.0  ...   4.671883  10.839150  1.806525   0.000000   \n",
       "863    11.0         4.0   7.0  ...  11.088465   6.732051  1.683013   2.244017   \n",
       "864    16.0         5.0  11.0  ...   4.568072   8.849874  1.769975   2.362437   \n",
       "865    51.0        17.0  34.0  ...   4.750215  32.828432  1.931084   2.414201   \n",
       "866    13.0         5.0   8.0  ...   5.850187   8.849874  1.769975   4.996320   \n",
       "...     ...         ...   ...  ...        ...        ...       ...        ...   \n",
       "1025    8.0         6.0   2.0  ...  42.711125  10.523646  1.753941   9.301424   \n",
       "1026    8.0         7.0   1.0  ...  18.992593  12.293055  1.756151  11.587713   \n",
       "1027   12.0         6.0   6.0  ...  10.332055  10.675851  1.779308   4.745721   \n",
       "1028   40.0        40.0   0.0  ...  16.773999  73.753328  1.843833  67.907675   \n",
       "1029   15.0         7.0   8.0  ...   9.466348  12.833789  1.833398   7.656092   \n",
       "1030   19.0         7.0  12.0  ...   5.478091  12.670365  1.810052   5.199368   \n",
       "1031   28.0        10.0  18.0  ...   5.076277  18.260598  1.826060   2.399340   \n",
       "1032    7.0         6.0   1.0  ...  20.844185  10.465990  1.744332   9.237437   \n",
       "1033   69.0        24.0  45.0  ...   4.918118  46.661553  1.944231   4.764176   \n",
       "1034   13.0         6.0   7.0  ...   8.462560  10.675851  1.779308  10.624436   \n",
       "1035   53.0        20.0  33.0  ...   5.815314  38.423081  1.921154  15.175200   \n",
       "1036   26.0        25.0   1.0  ...  15.922065  45.785614  1.831425  40.595154   \n",
       "1037   21.0         9.0  12.0  ...   5.718757  17.382017  1.931335   0.000000   \n",
       "1038   15.0        12.0   3.0  ...  32.107677  22.897487  1.908124  12.421431   \n",
       "1039   16.0         8.0   8.0  ...   6.754297  15.532829  1.941604   4.990326   \n",
       "1040   21.0        11.0  10.0  ...   7.146634  21.392597  1.944782   7.845920   \n",
       "1041   28.0        12.0  16.0  ...   5.861433  23.005284  1.917107   2.513691   \n",
       "1042   22.0        10.0  12.0  ...   6.185855  19.224956  1.922496   2.485848   \n",
       "1043   31.0        13.0  18.0  ...   5.746315  24.847459  1.911343   2.514398   \n",
       "1044   16.0         9.0   7.0  ...   8.876159  17.382017  1.931335   4.981112   \n",
       "1045   17.0         9.0   8.0  ...   8.237034  17.549594  1.949955   5.271016   \n",
       "1046   35.0        15.0  20.0  ...   5.832898  29.017579  1.934505   2.406054   \n",
       "1047   20.0        14.0   6.0  ...   9.901384  26.933327  1.923809  18.226967   \n",
       "1048   34.0        19.0  15.0  ...   8.766297  37.142943  1.954892  20.311353   \n",
       "1049   30.0        18.0  12.0  ...   8.135785  36.092058  2.005114  10.416198   \n",
       "1050   28.0        11.0  17.0  ...   5.540393  20.928005  1.902546   5.386377   \n",
       "1051   51.0        30.0  21.0  ...   8.296623  58.481230  1.949374  34.271881   \n",
       "1052   54.0        33.0  21.0  ...   8.206655  66.773197  2.023430  25.562011   \n",
       "1053   45.0        42.0   3.0  ...  13.265478   0.000000  0.000000   0.000000   \n",
       "1054   22.0        14.0   8.0  ...  10.090923  28.306077  2.021863   5.093771   \n",
       "\n",
       "         WTPT-4     WTPT-5   WPATH  WPOL   XLogP  Zagreb  \n",
       "837    0.000000   0.000000    56.0   4.0   4.323    22.0  \n",
       "838    2.500000   0.000000     3.0   0.0  -0.190    12.0  \n",
       "839    0.000000   0.000000    42.0   5.0   2.474    30.0  \n",
       "840    0.000000   2.926777   120.0   6.0   2.524    30.0  \n",
       "841    2.400011   0.000000   180.0  10.0   2.468    46.0  \n",
       "842    8.289840   0.000000   220.0   8.0   0.471    38.0  \n",
       "843    2.362437   0.000000    20.0   2.0   1.156    14.0  \n",
       "844    4.686998   0.000000    66.0   5.0   0.183    34.0  \n",
       "845   10.256638   0.000000   192.0  10.0   0.290    42.0  \n",
       "846   11.003383   0.000000  1794.0  27.0   6.934    98.0  \n",
       "847    7.279942   0.000000  1444.0  19.0   6.623    82.0  \n",
       "848    0.000000   2.414212  1330.0  17.0   9.388    74.0  \n",
       "849    0.000000   0.000000    88.0   9.0   2.281    40.0  \n",
       "850    5.273677   0.000000    86.0  10.0   0.975    40.0  \n",
       "851    2.452455   0.000000    64.0   7.0   0.593    34.0  \n",
       "852    8.039140   0.000000   198.0  16.0   0.910    54.0  \n",
       "853    4.818143   0.000000   120.0  11.0   1.374    46.0  \n",
       "854    4.817250   0.000000   117.0  11.0   1.374    46.0  \n",
       "855   15.757946   0.000000   721.0  23.0  -0.894    80.0  \n",
       "856    2.506669   2.454688    90.0   9.0   0.376    40.0  \n",
       "857    7.121372   0.000000   152.0  13.0  -0.032    54.0  \n",
       "858    5.232924   0.000000   125.0  11.0   0.670    44.0  \n",
       "859    7.712862   0.000000   482.0  19.0   2.655    72.0  \n",
       "860    0.000000   5.750000    15.0   0.0  -0.038    20.0  \n",
       "861    2.490697   2.944136    42.0   5.0  -0.122    30.0  \n",
       "862    0.000000   0.000000    35.0   3.0   3.388    18.0  \n",
       "863    0.000000   0.000000     9.0   0.0   2.033    12.0  \n",
       "864    0.000000   2.362437    20.0   2.0   0.784    14.0  \n",
       "865    2.414201   0.000000   816.0  14.0   7.679    62.0  \n",
       "866    4.996320   0.000000    20.0   2.0  -0.689    14.0  \n",
       "...         ...        ...     ...   ...     ...     ...  \n",
       "1025   0.000000   0.000000    29.0   4.0   3.928    22.0  \n",
       "1026   0.000000   0.000000    42.0   6.0   2.818    30.0  \n",
       "1027   0.000000   0.000000    31.0   4.0   2.025    20.0  \n",
       "1028   0.000000   3.307146  4131.0  99.0  11.748   228.0  \n",
       "1029   2.853553   0.000000    56.0   4.0   1.550    22.0  \n",
       "1030   5.199368   0.000000    52.0   4.0   0.194    24.0  \n",
       "1031   2.399340   0.000000   131.0   8.0   3.916    42.0  \n",
       "1032   2.332107   0.000000    28.0   3.0   1.444    24.0  \n",
       "1033   2.382088   2.382088  2279.0  21.0  10.256    92.0  \n",
       "1034   7.653238   0.000000    31.0   4.0  -0.742    20.0  \n",
       "1035  11.573295   0.000000  1006.0  21.0   5.067    80.0  \n",
       "1036   4.710243   0.000000  1248.0  60.0   5.905   138.0  \n",
       "1037   0.000000   0.000000    84.0  10.0   3.258    42.0  \n",
       "1038   0.000000   0.000000   174.0  21.0   4.762    60.0  \n",
       "1039   0.000000   4.990326    62.0   7.0  -0.168    36.0  \n",
       "1040   2.395580   5.450340   166.0  11.0  -0.097    50.0  \n",
       "1041   2.513691   0.000000   181.0  17.0   2.688    60.0  \n",
       "1042   2.485848   0.000000   110.0  13.0   1.700    48.0  \n",
       "1043   2.514398   0.000000   224.0  19.0   3.080    66.0  \n",
       "1044   2.489663   0.000000    84.0  10.0   1.704    42.0  \n",
       "1045   2.506669   0.000000    90.0   9.0   1.432    40.0  \n",
       "1046   2.406054   0.000000   402.0  19.0   4.112    72.0  \n",
       "1047  12.176220   6.050747   286.0  21.0   2.208    68.0  \n",
       "1048   8.752453   5.520549   760.0  25.0   3.629    86.0  \n",
       "1049  10.416198   0.000000   633.0  28.0   1.666    90.0  \n",
       "1050   2.490046   2.896331   138.0  13.0   1.003    58.0  \n",
       "1051  23.959978  10.311903  2679.0  45.0  -0.075   138.0  \n",
       "1052  10.402495  15.159515  4050.0  46.0   3.873   160.0  \n",
       "1053   0.000000   0.000000  4689.0  91.0   0.000   260.0  \n",
       "1054   0.000000   0.000000   287.0  20.0   2.962    70.0  \n",
       "\n",
       "[218 rows x 1423 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = biodeg[biodeg['Status'].str.contains('Test')]\n",
    "test_y = encode(test_x['Class'])\n",
    "\n",
    "# manipulate data type of x data to float\n",
    "test_x = test_x.iloc[:,4:1449]\n",
    "test_x.drop(test_x.select_dtypes(['object']), inplace=True, axis=1)\n",
    "test_x.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "qconstant_filter = VarianceThreshold(threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_filter.fit(train_x)\n",
    "len(train_x.columns[constant_filter.get_support()])\n",
    "constant_columns = [column for column in train_x.columns if column\n",
    "                   not in train_x.columns[constant_filter.get_support()]]\n",
    "\n",
    "train_x.drop(labels=constant_columns, axis=1, inplace=True)\n",
    "test_x.drop(labels=constant_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((837, 1010), (218, 1010))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qconstant_filter.fit(train_x)\n",
    "len(train_x.columns[qconstant_filter.get_support()])\n",
    "\n",
    "q_constant_columns = [column for column in train_x.columns if column\n",
    "                     not in train_x.columns[qconstant_filter.get_support()]]\n",
    "\n",
    "train_x = qconstant_filter.transform(train_x)\n",
    "test_x = qconstant_filter.transform(test_x)\n",
    "\n",
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = set()\n",
    "correlation_matrix = pd.DataFrame(train_x).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i,j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.DataFrame(train_x)\n",
    "test_x = pd.DataFrame(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "test_x.drop(labels=correlated_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "xscaler = StandardScaler().fit(train_x)\n",
    "train_x = xscaler.transform(train_x)\n",
    "testscaler = StandardScaler().fit(test_x)\n",
    "test_x = testscaler.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(x_train_res)\n",
    "y_train = pd.DataFrame(y_train_res)\n",
    "x_test = test_x\n",
    "y_test = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "tf.set_random_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases, keep_prob):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'hidden' : [10, 20, 30, 40, 50],\n",
    "             'prob' : [0.2, 0.4, 0.6, 0.8],\n",
    "             'size': [32, 45, 60],\n",
    "             'rate' : [0.05, 0.01, 0.001, 0.001]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(param_dist):\n",
    "    n_hidden_1 = hidden\n",
    "    n_input = train_x.shape[1]\n",
    "    n_classes = train_y.shape[1]\n",
    "\n",
    "    weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "    training_epochs = 200\n",
    "    display_step = 90\n",
    "    batch_size = size\n",
    "\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=rate).minimize(cost)\n",
    "\n",
    "    return\n",
    "\n",
    "def run_model(setup_model):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.0\n",
    "            total_batch = int(len(x_train) / batch_size)\n",
    "            x_batches = np.array_split(x_train, total_batch)\n",
    "            y_batches = np.array_split(y_train, total_batch)\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "                _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: prob\n",
    "                            })\n",
    "                avg_cost += c / total_batch\n",
    "            if epoch % display_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "        print(\"Optimization Finished!\")\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    \n",
    "        confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "        print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "        \n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y, predictions)))\n",
    "        R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "        print(R_squared.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "        \n",
    "        return hidden, prob, size, rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (62, 1) for Tensor 'Placeholder_476:0', which has shape '(?, 2)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-fda32409cd34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m                                \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                                \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                                \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.35\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                            })\n\u001b[0;32m     46\u001b[0m            \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1128\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1129\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (62, 1) for Tensor 'Placeholder_476:0', which has shape '(?, 2)'"
     ]
    }
   ],
   "source": [
    " #### use meeeeee \n",
    "n_hidden_1 = 60\n",
    "n_input = train_x.shape[1]\n",
    "n_classes = train_y.shape[1]\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "training_epochs = 10\n",
    "display_step = 1\n",
    "batch_size = 60\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        x_batches = np.array_split(x_train, total_batch)\n",
    "        y_batches = np.array_split(y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: 0.35\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    \n",
    "    confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "    print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x = train_x.values\n",
    "train_y = train_y.values\n",
    "\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=12, ratio = 1.0, )\n",
    "x_train_res, y_train_res = sm.fit_sample(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-297155e4b118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-527f28c963be>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(setup_model)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mavg_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mtotal_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "estimator = run_model(setup_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'hidden' : [10, 20, 30, 40, 50],\n",
    "             'prob' : [0.2, 0.4, 0.6, 0.8],\n",
    "             'size': [32, 45, 60],\n",
    "             'rate' : [0.05, 0.01, 0.001, 0.001]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    " = [30, 35, 40, 45, 50, 55, 60, 65, 70]\n",
    "pro = [0.3, 0.35, 0.4]\n",
    "rat = [0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for hidden in hid:\n",
    "    for prob in pro:\n",
    "        for rate in rat:\n",
    "                \n",
    "            mod = []\n",
    "                    \n",
    "            n_hidden_1 = hidden\n",
    "            n_input = train_x.shape[1]\n",
    "            n_classes = train_y.shape[1]\n",
    "            weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "            biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "            keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "            training_epochs = 200\n",
    "            display_step = 100\n",
    "            batch_size = 60\n",
    "\n",
    "            x = tf.placeholder(\"float\", [None, n_input])\n",
    "            y = tf.placeholder(\"float\", [None, n_classes])\n",
    "            predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=rate).minimize(cost)\n",
    "            \n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "                for epoch in range(training_epochs):\n",
    "                    avg_cost = 0.0\n",
    "                    total_batch = int(len(x_train) / batch_size)\n",
    "                    x_batches = np.array_split(x_train, total_batch)\n",
    "                    y_batches = np.array_split(y_train, total_batch)\n",
    "                for i in range(total_batch):\n",
    "                    batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "                    _, c = sess.run([optimizer, cost], \n",
    "                                feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: prob\n",
    "                            })\n",
    "                    avg_cost += c / total_batch\n",
    "                if epoch % display_step == 0:\n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                    \"{:.9f}\".format(avg_cost))\n",
    "                correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "                mod.append(accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "                \n",
    "    \n",
    "                confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "                mod.append(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "                mod.append(hidden)\n",
    "                mod.append(prob)\n",
    "                mod.append(size)\n",
    "                mod.append(rate)\n",
    "                \n",
    "                acc.append(mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.72477067, array([[145,   1],\n",
       "         [ 59,  13]]), 70, 0.4, 60, 0.1], [0.7477064, array([[129,  17],\n",
       "         [ 38,  34]]), 35, 0.35, 60, 0.1], [0.75688076, array([[111,  35],\n",
       "         [ 18,  54]]), 35, 0.3, 60, 0.1], [0.7798165, array([[125,  21],\n",
       "         [ 27,  45]]), 50, 0.35, 60, 0.1], [0.7844037, array([[137,   9],\n",
       "         [ 38,  34]]), 65, 0.35, 60, 0.1], [0.78899086, array([[129,  17],\n",
       "         [ 29,  43]]), 30, 0.35, 60, 0.1], [0.78899086, array([[116,  30],\n",
       "         [ 16,  56]]), 35, 0.4, 60, 0.1], [0.79357797, array([[128,  18],\n",
       "         [ 27,  45]]), 45, 0.35, 60, 0.1], [0.79357797, array([[125,  21],\n",
       "         [ 24,  48]]), 70, 0.3, 60, 0.1], [0.79816514, array([[136,  10],\n",
       "         [ 34,  38]]), 30, 0.4, 60, 0.1], [0.79816514, array([[132,  14],\n",
       "         [ 30,  42]]), 60, 0.4, 60, 0.1], [0.79816514, array([[134,  12],\n",
       "         [ 32,  40]]), 65, 0.4, 60, 0.1], [0.8027523, array([[122,  24],\n",
       "         [ 19,  53]]), 30, 0.3, 60, 0.1], [0.8027523, array([[120,  26],\n",
       "         [ 17,  55]]), 45, 0.3, 60, 0.1], [0.8027523, array([[135,  11],\n",
       "         [ 32,  40]]), 55, 0.35, 60, 0.1], [0.8027523, array([[131,  15],\n",
       "         [ 28,  44]]), 55, 0.4, 60, 0.1], [0.8027523, array([[129,  17],\n",
       "         [ 26,  46]]), 60, 0.3, 60, 0.1], [0.8027523, array([[131,  15],\n",
       "         [ 28,  44]]), 65, 0.3, 60, 0.1], [0.8027523, array([[132,  14],\n",
       "         [ 29,  43]]), 70, 0.35, 60, 0.1], [0.80733943, array([[134,  12],\n",
       "         [ 30,  42]]), 40, 0.4, 60, 0.1], [0.80733943, array([[131,  15],\n",
       "         [ 27,  45]]), 45, 0.4, 60, 0.1], [0.80733943, array([[128,  18],\n",
       "         [ 24,  48]]), 50, 0.3, 60, 0.1], [0.8119266, array([[124,  22],\n",
       "         [ 19,  53]]), 40, 0.3, 60, 0.1], [0.8119266, array([[133,  13],\n",
       "         [ 28,  44]]), 50, 0.4, 60, 0.1], [0.8119266, array([[125,  21],\n",
       "         [ 20,  52]]), 55, 0.3, 60, 0.1], [0.8211009, array([[126,  20],\n",
       "         [ 19,  53]]), 60, 0.35, 60, 0.1], [0.8394495, array([[131,  15],\n",
       "         [ 20,  52]]), 40, 0.35, 60, 0.1]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(acc, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.7706422, array([[118,  28], [ 22,  50]]), 10, 0.7, 60, 0.1], \n",
    "[0.80733943, array([[131,  15], [ 27,  45]]), 50, 0.4, 60, 0.1], # best learning rate: 0.1, best batch size: 60\n",
    "[0.80733943, array([[133,  13], [ 29,  43]]), 50, 0.7, 60, 0.1], # need to explore hidden layers and keep prob\n",
    "[0.8211009, array([[125,  21], [ 18,  54]]), 30, 0.7, 60, 0.1], \n",
    "[0.853211, array([[127,  19], [ 13,  59]]), 30, 0.4, 60, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.7706422, array([[110,  36],\n",
    "         [ 14,  58]]), 10, 0.7, 60, 0.1], \n",
    "[0.77522933, array([[117,  29],\n",
    "         [ 20,  52]]), 30, 0.5, 60, 0.1], \n",
    "[0.77522933, array([[127,  19],\n",
    "         [ 30,  42]]), 70, 0.7, 60, 0.1], \n",
    "[0.7844037, array([[131,  15],\n",
    "         [ 32,  40]]), 50, 0.3, 60, 0.1], \n",
    "[0.7844037, array([[133,  13],\n",
    "         [ 34,  38]]), 50, 0.7, 60, 0.1], # best 0.1 learning rate, dropout 0.5 or 0.3, best hidden unknown...\n",
    "[0.78899086, array([[129,  17],\n",
    "         [ 29,  43]]), 30, 0.7, 60, 0.1], \n",
    "[0.79816514, array([[113,  33],\n",
    "         [ 11,  61]]), 10, 0.5, 60, 0.1], \n",
    "[0.79816514, array([[138,   8],\n",
    "         [ 36,  36]]), 50, 0.5, 60, 0.1], \n",
    "[0.8027523, array([[119,  27],\n",
    "         [ 16,  56]]), 30, 0.3, 60, 0.1], \n",
    "[0.8027523, array([[137,   9],\n",
    "         [ 34,  38]]), 70, 0.3, 60, 0.1], \n",
    "[0.8348624, array([[128,  18],\n",
    "         [ 18,  54]]), 70, 0.5, 60, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " [0.83027524, array([[132,  14],\n",
    "         [ 23,  49]]), 50, 0.3, 60, 0.1]] # narrow in on 50 and 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.8211009, array([[126,  20],\n",
    "         [ 19,  53]]), 60, 0.35, 60, 0.1], winner \n",
    "hyperparameters: 60, .35, 60, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "target must be a string, but got <class 'function'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m    652\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[1;32m---> 61\u001b[1;33m                     (bytes_or_text,))\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected binary or unicode string, got <function create_model at 0x000001C775D1D620>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-80219cfb327f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m     \u001b[1;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'target must be a string, but got %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: target must be a string, but got <class 'function'>"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "X, Y = x_train, y_train\n",
    "\n",
    "# Create model for KerasClassifier\n",
    "def create_model(hparams1,\n",
    "                 hparams2,\n",
    "                 hparams3,\n",
    "                 hparams4):\n",
    "    \n",
    "    n_hidden_1 = hparams1\n",
    "    n_input = train_x.shape[1]\n",
    "    n_classes = train_y.shape[1]\n",
    "\n",
    "    weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "    training_epochs = 200\n",
    "    display_step = 90\n",
    "    batch_size = hparams3\n",
    "\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hparams4).minimize(cost)\n",
    "\n",
    "    return\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        x_batches = np.array_split(x_train, total_batch)\n",
    "        y_batches = np.array_split(y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                        feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: hparams2\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "            \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    \n",
    "    confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "    print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "hparams1 = randint(10, 100)\n",
    "hparams2 = randint(0,10)*0.1\n",
    "hparams3 = randint(30,60)\n",
    "hparams4 = [0.1, 0.05, 0.01, 0.001]\n",
    "\n",
    "# Prepare the Dict for the Search\n",
    "param_dist = dict(hparams1=hparams1, \n",
    "                  hparams2=hparams2, \n",
    "                  hparams3=hparams3, \n",
    "                  hparams4=hparams4)\n",
    "\n",
    "# Search in action!\n",
    "n_iter_search = 16 # Number of parameter settings that are sampled.\n",
    "random_search = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(X, Y)\n",
    "\n",
    "# Show the results\n",
    "print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "means = random_search.cv_results_['mean_test_score']\n",
    "stds = random_search.cv_results_['std_test_score']\n",
    "params = random_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 0 but is rank 1 for 'Adam_1/update_Variable_4/ApplyAdam' (op: 'ApplyAdam') with input shapes: [295,98], [295,98], [295,98], [], [], [4], [], [], [], [295,98].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1658\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1659\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Shape must be rank 0 but is rank 1 for 'Adam_1/update_Variable_4/ApplyAdam' (op: 'ApplyAdam') with input shapes: [295,98], [295,98], [295,98], [], [], [4], [], [], [], [295,98].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-7be36ad361bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhparams4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[1;32m--> 413\u001b[1;33m                                 name=name)\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    610\u001b[0m           \u001b[0mscope_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"update_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m           \u001b[0mupdate_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mupdate_op\u001b[1;34m(self, optimizer, g)\u001b[0m\n\u001b[0;32m    117\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\u001b[0m in \u001b[0;36m_apply_dense\u001b[1;34m(self, grad, var)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_beta2_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epsilon_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         grad, use_locking=self._use_locking).op\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_resource_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\gen_training_ops.py\u001b[0m in \u001b[0;36mapply_adam\u001b[1;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[0;32m    300\u001b[0m                      \u001b[0mbeta2_power\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta2_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                      \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                      use_nesterov=use_nesterov, name=name)\n\u001b[0m\u001b[0;32m    303\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    789\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3300\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1823\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1660\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1662\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1664\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape must be rank 0 but is rank 1 for 'Adam_1/update_Variable_4/ApplyAdam' (op: 'ApplyAdam') with input shapes: [295,98], [295,98], [295,98], [], [], [4], [], [], [], [295,98]."
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "# Create model for KerasClassifier\n",
    "   \n",
    "n_hidden_1 = hparams1\n",
    "n_input = train_x.shape[1]\n",
    "n_classes = train_y.shape[1]\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "    }\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "training_epochs = 200\n",
    "display_step = 90\n",
    "batch_size = hparams3\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=hparams4).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        x_batches = np.array_split(x_train, total_batch)\n",
    "        y_batches = np.array_split(y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                        feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: hparams2\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "            \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    \n",
    "    confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "    print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "\n",
    "# Specify parameters and distributions to sample from\n",
    "hparams1 = randint(10, 100)\n",
    "hparams2 = randint(0,10)*0.1\n",
    "hparams3 = randint(30,60)\n",
    "hparams4 = [0.1, 0.05, 0.01, 0.001]\n",
    "\n",
    "# Prepare the Dict for the Search\n",
    "param_dist = dict(hparams1=hparams1, \n",
    "                  hparams2=hparams2, \n",
    "                  hparams3=hparams3, \n",
    "                  hparams4=hparams4)\n",
    "\n",
    "# Search in action!\n",
    "n_iter_search = 16 # Number of parameter settings that are sampled.\n",
    "random_search = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(X, Y)\n",
    "\n",
    "# Show the results\n",
    "print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "means = random_search.cv_results_['mean_test_score']\n",
    "stds = random_search.cv_results_['std_test_score']\n",
    "params = random_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_val, Y_val):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense({{choice([10, 20, 40, 104])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([10, 20, 40, 104])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    if conditional({{choice(['two', 'three'])}}) == 'three':\n",
    "        model.add(Dense({{choice([10, 20, 40, 104])}}))\n",
    "        model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "        \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    adam = keras.optimizers.Adam(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "    rmsprop = keras.optimizers.RMSprop(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "    sgd = keras.optimizers.SGD(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "   \n",
    "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
    "    if choiceval == 'adam':\n",
    "        optim = adam\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = rmsprop\n",
    "    else:\n",
    "        optim = sgd\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size={{choice([128,256,512])}},\n",
    "              nb_epoch=20,\n",
    "              verbose=2,\n",
    "              validation_data=(X_val, Y_val))\n",
    "    score, acc = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from math import floor, ceil\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from pylab import rcParams\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils import shuffle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_selection import VarianceThreshold\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from random import randint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import RandomizedSearchCV\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [10, 20, 40, 104]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense_1': hp.choice('Dense_1', [10, 20, 40, 104]),\n",
      "        'Activation_1': hp.choice('Activation_1', ['relu', 'sigmoid']),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'conditional': hp.choice('conditional', ['two', 'three']),\n",
      "        'Dense_2': hp.choice('Dense_2', [10, 20, 40, 104]),\n",
      "        'Activation_2': hp.choice('Activation_2', ['relu', 'sigmoid']),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'lr': hp.choice('lr', [10**-3, 10**-2, 10**-1]),\n",
      "        'lr_1': hp.choice('lr_1', [10**-3, 10**-2, 10**-1]),\n",
      "        'lr_2': hp.choice('lr_2', [10**-3, 10**-2, 10**-1]),\n",
      "        'choiceval': hp.choice('choiceval', ['adam', 'sgd', 'rmsprop']),\n",
      "        'batch_size': hp.choice('batch_size', [128,256,512]),\n",
      "    }\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "module, class, method, function, traceback, frame, or code object was expected, got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-77c3760a373e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                                       \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                       \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                                       notebook_name='Neural Network-Copy1')\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mmodel_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_model_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mmodel_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_hyperopt_model_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[0mtemp_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./temp_model.py'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mwrite_temp_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mget_hyperopt_model_string\u001b[1;34m(model, data, functions, notebook_name, verbose, stack)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0mfunctions_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretrieve_function_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[0mdata_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretrieve_data_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperopt_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maug_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mretrieve_data_string\u001b[1;34m(data, verbose)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mretrieve_data_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0mdata_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m     \u001b[0mfirst_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[0mindent_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetermine_indent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mAn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     OSError is raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m--> 968\u001b[1;33m     \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetsourcelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsourcelines\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    953\u001b[0m     raised if the source code cannot be retrieved.\"\"\"\n\u001b[0;32m    954\u001b[0m     \u001b[0mobject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 955\u001b[1;33m     \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mismodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    766\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Invalidate cache if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0mway\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m     \"\"\"\n\u001b[1;32m--> 684\u001b[1;33m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEBUG_BYTECODE_SUFFIXES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZED_BYTECODE_SUFFIXES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetfile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    664\u001b[0m     raise TypeError('module, class, method, function, traceback, frame, or '\n\u001b[0;32m    665\u001b[0m                     'code object was expected, got {}'.format(\n\u001b[1;32m--> 666\u001b[1;33m                     type(object).__name__))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetmodulename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: module, class, method, function, traceback, frame, or code object was expected, got tuple"
     ]
    }
   ],
   "source": [
    "data = X_train, Y_train, X_val, Y_val\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=30,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Neural Network-Copy1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to make this different... this needs to have a different train/test layout because the test data is not getting called properly. try https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428 this method here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predictions, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n",
    "\n",
    "\n",
    "    confusion = tf.confusion_matrix(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1), num_classes=2) \n",
    "    print(confusion.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " total_error = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "    unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y, predictions)))\n",
    "    R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "    print(R_squared.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn naive random oversampling (imbalanced data)\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.tensorflow.balanced_batch_generator.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* look for network architecture from paper that used the QSAR \n",
    "* lasso \n",
    "* fix for sparse data\n",
    "* find any columns that are uniform (or very low variation)\n",
    "* normalize \n",
    "* tensorboard\n",
    "* early stopping - ask rainie if i need help\n",
    "* put layer in after dropout\n",
    "* if oversampling, up the dropout (is there a ratio)\n",
    "* test set needs to be balanced but not oversampled \n",
    "* use their train/test split, then shuffle the data\n",
    "* combine the set up cells and do a for loop for the number of nodes (like 5-50 at 5 or 10 node increments)\n",
    "    * use on waffle?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create environment (homoganize)\n",
    "install everything i need into it\n",
    "* do the wget thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    MSE = tf.metrics.mean_squared_error(tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    print(\"MSE:\", MSE)\n",
    "    fn = tf.metrics.false_negatives(\n",
    "    tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    tn = tf.metrics.true_negatives(\n",
    "    tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    fp = tf.metrics.false_positives(\n",
    "    tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    tp = tf.metrics.true_positives(\n",
    "    tf.cast(y_test, tf.float32),\n",
    "    predictions,\n",
    "    weights=None,\n",
    "    metrics_collections=None,\n",
    "    updates_collections=None,\n",
    "    name=None)\n",
    "    print(\"FN:\", fn, \"TN:\", tn, \"FP:\", fp, \"TP:\", tp)\n",
    "    total_error = tf.reduce_sum(tf.square(tf.subtract(tf.cast(y_test, tf.float32), tf.reduce_mean(tf.cast(y_test, tf.float32)))))\n",
    "    unexplained_error = tf.reduce_sum(tf.square(tf.subtract(tf.cast(y_test, tf.float32), tf.cast(predictions, tf.float32))))\n",
    "    R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error))\n",
    "    print(R_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n",
    "...                    'num_wings': [2, 0, 0, 0],\n",
    "...                    'num_specimen_seen': [10, 2, 1, 8]},\n",
    "...                   index=['falcon', 'dog', 'spider', 'fish'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1 = df.iloc[:,1:3]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
